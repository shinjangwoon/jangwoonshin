{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Learning 연습 ( 데이터 전처리).ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNvMUCkB9QtHC9HbFYJx26J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shinjangwoon/jangwoonshin/blob/practice/Machine_Learning_%EC%97%B0%EC%8A%B5_(_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%A0%84%EC%B2%98%EB%A6%AC).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrygA3Df2LMM"
      },
      "source": [
        "#데이터 전처리\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxCqFqkaE_ez"
      },
      "source": [
        "##인코딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rZOYJOSO6Nh"
      },
      "source": [
        "'''\n",
        "-------- [최종 출력 결과] --------\n",
        "LabelEncoding classes: ***\n",
        "LabelDecoding result: ***\n",
        "OneHotEncoding classes: ***\n",
        "----------------------------------\n",
        "'''\n",
        "# 필요한 라이브러리 로딩\n",
        "import seaborn as sns\n",
        "\n",
        "# sklearn 의 LabelEncoder, OneHotEncoder 를 import 시키기\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "#데이터 로딩\n",
        "tips = sns.load_dataset('tips')\n",
        "print(tips.head(5))\n",
        "# 인코딩할 컬럼 데이터 준비 \n",
        "items = tips['day']\n",
        "\n",
        "# 1. 라벨인코딩(LabelEncoding) 실습\n",
        "# LabelEncoder 객체 생성\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "# fit 메소드에 인코딩할 데이터 전달\n",
        "encoder.fit(items)\n",
        "\n",
        "# transform 메소드를 통해 데이터 변환 \n",
        "labels = encoder.transform(items)\n",
        "\n",
        "# 인코딩 결과 출력 \n",
        "print('Label Encoding Result:\\n',labels)\n",
        "\n",
        "# 인코딩된 수치형 데이터의 실제 클래스 확인 및 출력\n",
        "classes = encoder.classes_\n",
        "print('LabelEncoding classes:', classes)\n",
        "\n",
        "# 디코딩 결과 확인 및 출력\n",
        "inverse_result = encoder.inverse_transform([2])\n",
        "print('LabelDecoding result:', inverse_result)\n",
        "\n",
        "# 2. 원핫인코딩(OneHotEncoding) 실습\n",
        "# 2차원 데이터로 변환\n",
        "labels = labels.reshape(-1,1)\n",
        "\n",
        "# OneHotEncoder 객체 생성\n",
        "one_hot_encoder = OneHotEncoder()\n",
        "\n",
        "# .fit 메소드에 인코딩할 데이터 전달\n",
        "one_hot_encoder.fit(labels)\n",
        "\n",
        "# .transform 메소드를 통해 데이터 변환 \n",
        "one_hot_labels = one_hot_encoder.transform(labels)\n",
        "\n",
        "# 인코딩 결과 출력 \n",
        "print('OneHotEncoding Result:\\n', one_hot_labels.toarray())\n",
        "\n",
        "# 속성 이용하여 인코딩된 데이터의 실제 클래스 확인\n",
        "onehot_classes = one_hot_encoder.categories_\n",
        "print('OneHotEncoding classes:', onehot_classes)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrRdQzY42T3M"
      },
      "source": [
        "##스케일링"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBjTyhs_2c-_"
      },
      "source": [
        "'''\n",
        "-------- [최종 출력 결과] --------\n",
        "Average\n",
        "temp         ***\n",
        "atemp        ***\n",
        "humidity     ***\n",
        "windspeed    ***\n",
        "dtype: float64\n",
        "Variance\n",
        "temp          ***\n",
        "atemp         ***\n",
        "humidity     ***\n",
        "windspeed     ***\n",
        "dtype: float64\n",
        "--------- StandardScaler ---------\n",
        "Average\n",
        "temp         ***\n",
        "atemp       ***\n",
        "humidity    ***\n",
        "windspeed   ***\n",
        "dtype: float64\n",
        "Variance\n",
        "temp         ***\n",
        "atemp        ***\n",
        "humidity     ***\n",
        "windspeed    ***\n",
        "dtype: float64\n",
        "----------------------------------\n",
        "'''\n",
        "# 필요한 라이브러리 로딩\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# StandardScaler 로딩\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# URL 통해서 캐글의 자전거 대여 수요 데이터셋 다운로드\n",
        "url = 'https://codepresso-online-platform-public.s3.ap-northeast-2.amazonaws.com/learning-resourse/python-machine-learning-20210326/bike-demand.csv'\n",
        "df_bike = pd.read_csv(url)\n",
        "print(df_bike.head(5))\n",
        "\n",
        "# temp, atemp, humidity, windspeed\t컬럼 데이터만 저장\n",
        "df_bike_num = df_bike.iloc[:, 5:9]\n",
        "\n",
        "print(df_bike_num.head(5))\n",
        "\n",
        "# 각 컬럼별 평균, 분산 출력\n",
        "print('Average')\n",
        "print(np.round_(df_bike_num.mean(),3))\n",
        "print('Variance')\n",
        "print(np.round_(df_bike_num.var(),3)) \n",
        "\n",
        "# StandardScaler 객체 생성\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# StandardScaler 모델 통해 데이터 분포 분석\n",
        "scaler.fit(df_bike_num)\n",
        "\n",
        "# 모델 통해서 데이터 스케일링 후 반환\n",
        "result = scaler.transform(df_bike_num)\n",
        "\n",
        "# 스케일된 결과 데이터를 DataFrame 으로 저장\n",
        "scaled_bike = pd.DataFrame(data=result, columns=df_bike_num.columns)\n",
        "\n",
        "# 각 컬럼별 평균, 분산 출력\n",
        "print('--------- StandardScaler ---------')\n",
        "print('Average')\n",
        "print(np.round_(scaled_bike.mean(),3))\n",
        "print('Variance')\n",
        "print(np.round_(scaled_bike.var(),3)) \n",
        "# 박스플롯(boxplot)으로 시각화\n",
        "plt.figure(figsize=(10, 6))\n",
        "scaled_bike.boxplot(column=['temp', 'atemp', 'humidity', 'windspeed'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPVXsNtZE1sW"
      },
      "source": [
        "###스케일링 실습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FneR79opE5Gj"
      },
      "source": [
        "'''\n",
        "\n",
        "-------- [최종 출력 결과] --------\n",
        "Min Value\n",
        "temp         ***\n",
        "atemp        ***\n",
        "humidity     ***\n",
        "windspeed    ***\n",
        "dtype: float64\n",
        "Max Value\n",
        "temp          ***\n",
        "atemp         ***\n",
        "humidity     ***\n",
        "windspeed     ***\n",
        "dtype: float64\n",
        "--------- MinMaxScaler ---------\n",
        "Min Value\n",
        "temp         ***\n",
        "atemp       ***\n",
        "humidity    ***\n",
        "windspeed   ***\n",
        "dtype: float64\n",
        "Max Value\n",
        "temp         ***\n",
        "atemp        ***\n",
        "humidity     ***\n",
        "windspeed    ***\n",
        "dtype: float64\n",
        "----------------------------------\n",
        "'''\n",
        "# 필요한 라이브러리 로딩\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# MinMaxScaler 로딩\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# URL 통해서 캐글의 자전거 대여 수요 데이터셋 다운로드\n",
        "url = 'https://codepresso-online-platform-public.s3.ap-northeast-2.amazonaws.com/learning-resourse/python-machine-learning-20210326/bike-demand.csv'\n",
        "df_bike = pd.read_csv(url)\n",
        "print(df_bike.head(5))\n",
        "\n",
        "# temp, atemp, humidity, windspeed\t컬럼 데이터만 저장\n",
        "df_bike_num = df_bike.iloc[:, 5:9]\n",
        "\n",
        "print(df_bike_num.head(5))\n",
        "\n",
        "# 각 컬럼별 최대/최소값 출력\n",
        "print('Min Value')\n",
        "print(np.round_(df_bike_num.min(),3))\n",
        "print('Max Value')\n",
        "print(np.round_(df_bike_num.max(),3)) \n",
        "\n",
        "# MinMaxScaler 객체 생성\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# fit 함수 이용하여 데이터 분포 분석 및 스케일링 정보 저장\n",
        "scaler.fit(df_bike_num)\n",
        "\n",
        "# 모델 통해서 데이터 스케일링 후 반환\n",
        "result = scaler.transform(df_bike_num)\n",
        "\n",
        "# 스케일된 결과 데이터를 DataFrame 으로 저장\n",
        "scaled_bike = pd.DataFrame(data=result, columns=df_bike_num.columns)\n",
        "\n",
        "# 각 컬럼별 최대/최소값 출력\n",
        "print('--------- MinMaxScaler ---------')\n",
        "print('Min Value')\n",
        "print(np.round_(scaled_bike.min(),3))\n",
        "print('Max Value')\n",
        "print(np.round_(scaled_bike.max(),3)) \n",
        "# 박스플롯(boxplot)으로 시각화\n",
        "plt.figure(figsize=(10, 6))\n",
        "scaled_bike.boxplot(column=['temp', 'atemp', 'humidity', 'windspeed'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHyuFPJ2bfkc"
      },
      "source": [
        "##회귀(regression) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAUBJfIJbkY9"
      },
      "source": [
        "'''\n",
        "\n",
        "-------- [최종 출력 결과] --------\n",
        "Data shape: ***\n",
        "X shape: ***\n",
        "y shape: ***\n",
        "X reshape result: ***\n",
        "y reshape result: ***\n",
        "Testing data shape: ***\n",
        "----------------------------------\n",
        "'''\n",
        "# 필요한 라이브러리 로딩\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Boston housing price 데이터셋 로딩\n",
        "boston = load_boston()\n",
        "print('Data shape:', boston.data.shape)\n",
        "\n",
        "# 독립변수, 종속변수 데이터 정의\n",
        "X_rooms = boston.data[:, 5]\n",
        "y = boston.target\n",
        "print('X shape:', X_rooms.shape)\n",
        "print('y shape:', y.shape)\n",
        "\n",
        "# 데이터의 분포 확인을 위해 산점도로 시각화\n",
        "plt.figure()\n",
        "plt.scatter(X_rooms, y)\n",
        "plt.xlabel('Number of rooms')\n",
        "plt.ylabel('Values of house')\n",
        "plt.show()\n",
        "\n",
        "# 2차원 데이터로 shape 변환\n",
        "X_rooms = X_rooms.reshape(-1, 1)\n",
        "y = y.reshape(-1, 1)\n",
        "print('X reshape result:', X_rooms.shape)\n",
        "print('y reshape result:' , y.shape)\n",
        "\n",
        "# LinearRegression 객체 생성\n",
        "regression = LinearRegression()\n",
        "\n",
        "# 학습데이터 연결 및 학습 수행\n",
        "regression.fit(X_rooms, y)\n",
        "\n",
        "# 테스팅에 사용할 데이터 생성\n",
        "testing = np.linspace(min(X_rooms), max(X_rooms)).reshape(-1, 1)\n",
        "print('Testing data shape:', testing.shape)\n",
        "\n",
        "# 모델 예측 수행\n",
        "y_pred = regression.predict(testing)\n",
        "\n",
        "# 최적의 회귀선 시각화\n",
        "plt.figure(figsize=(10, 5))\n",
        "# plt.scatter 이용해서 산점도 시각화\n",
        "plt.scatter(X_rooms, y)\n",
        "# plt.plot 이용해서 라인 그래프 시각화\n",
        "plt.plot(testing, y_pred, color='red', Linewidth=3)\n",
        "plt.xlabel('Number of rooms')\n",
        "plt.ylabel('Values of house')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is79cPQ301Et"
      },
      "source": [
        "###선형 회귀"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG3IWUTi05ep"
      },
      "source": [
        "'''\n",
        "\n",
        "-------- [최종 출력 결과] --------\n",
        "Weight : ***\n",
        "Bias : ***\n",
        "\n",
        "MSE  : ***\n",
        "MAE  : ***\n",
        "RMSE : ***\n",
        "MAPE : ***\n",
        "\n",
        "R-squared(r2_score) : ***\n",
        "R-squared(r2_metric) : ***\n",
        "----------------------------------\n",
        "'''\n",
        "# 필요한 라이브러리 로딩\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "# 모델 성능 평가를 위한 metrics 모듈 로딩\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "# 데이터셋 로딩\n",
        "boston = load_boston()\n",
        "\n",
        "# 데이터셋 분할\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(boston.data , boston.target, test_size=0.3, random_state=12)\n",
        "                             \n",
        "# LinearRegression 객체 생성\n",
        "regression = LinearRegression()\n",
        "\n",
        "# 학습데이터 연결 및 학습 수행\n",
        "regression.fit(x_train, y_train)\n",
        "\n",
        "# 모델 예측\n",
        "y_pred = regression.predict(x_test)\n",
        "\n",
        "# 회귀 계수 출력\n",
        "weight = np.round(regression.coef_, 1)\n",
        "bias = np.round(regression.intercept_, 2)\n",
        "print('Weight:', weight)\n",
        "print('Bias:', bias)\n",
        "\n",
        "# 컬럼별 회귀계수 출력\n",
        "coef_table = pd.Series(data=weight,\n",
        "                        index=boston.feature_names)\n",
        "\n",
        "\n",
        "#print('Regression Coefficients :')\n",
        "#print(coef_table.sort_values(ascending=False))\n",
        "\n",
        "# 회귀 분석 모델을 위한 평가 지표 계산\n",
        "mse = metrics.mean_squared_error(y_test, y_pred)\n",
        "mae = metrics.mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
        "#mape = metrics.mean_absolute_percentage_error(y_test, y_pred)\n",
        "\n",
        "print('MSE  : {0:.3f}'.format(mse))\n",
        "print('MAE  : {0:.3f}'.format(mae))\n",
        "print('RMSE : {0:.3f}'.format(rmse))\n",
        "#print('MAPE : {0:.3f}'.format(mape))\n",
        "\n",
        "# R-squared 를 통한 모델의 설명력 평가\n",
        "r2_score = regression.score(x_test, y_test)\n",
        "r2_metric = metrics.r2_score(y_test, y_pred)\n",
        "\n",
        "print('\\nR-squared(r2_score) : {0:.3f}'.format(r2_score))\n",
        "print('R-squared(r2_metric) : {0:.3f}'.format(r2_metric))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX1WX_a301BU"
      },
      "source": [
        "### Ridge Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9UvbmgUBgXb"
      },
      "source": [
        "'''\n",
        "강의를 수강하시면서 하단 빈칸(_____)에 코드를 채워보세요.\n",
        "주석에 추가 가이드 정보를 기재하였습니다.\n",
        "최종 코드의 결과는 다음의 값만 출력하시고, \n",
        "제출버튼을 눌러 제출하시면 됩니다.\n",
        "\n",
        "※최종 코드는 반드시 alpha=0.1 로 지정한 실행 결과를 제출 부탁드립니다.\n",
        "-------- [최종 출력 결과] --------\n",
        "Training-datasset R2 :\n",
        "Test-datasset R2 :\n",
        "Ridge Regression Coefficients :\n",
        "----------------------------------\n",
        "'''\n",
        "# 필요한 라이브러리 로딩\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "# 릿지 회귀 모델 적용을 위해 Ridge 로딩\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# 데이터셋 로딩\n",
        "boston = load_boston()\n",
        "\n",
        "# 데이터셋 분할\n",
        "# random_state 값은 강의와 동일하게 지정하세요.\n",
        "x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.3, random_state=12)\n",
        "\n",
        "# 규제를 위한 alpha 값 초기화\n",
        "alpha = 0.1\n",
        "\n",
        "# Rigde 클래스 객체 생성\n",
        "ridge = Ridge(alpha=alpha)\n",
        "\n",
        "# 규제 학습 수행\n",
        "ridge.fit(x_train, y_train)\n",
        "\n",
        "# 모델을 통한 예측\n",
        "ridge_pred = ridge.predict(x_test)\n",
        "\n",
        "# 학습된 모델에 대한 R^2 계산\n",
        "r2_train = ridge.score(x_train, y_train)\n",
        "r2_test = ridge.score(x_test, y_test)\n",
        "print('Training-datasset R2 : {0:.3f}'.format(r2_train))\n",
        "print('Test-datasset R2 : {0:.3f}'.format(r2_test))\n",
        "\n",
        "# 컬럼별 회귀계수 저장한 Series 객체 생성 및 출력              \n",
        "ridge_coef_table = pd.Series(data=np.round(ridge.coef_, 1), index=boston.feature_names)\n",
        "print('Ridge Regression Coefficients :')\n",
        "print(ridge_coef_table.sort_values(ascending=False))\n",
        "\n",
        "# 막대그래프 시각화 \n",
        "plt.figure(figsize=(10, 5))\n",
        "ridge_coef_table.plot(kind='bar')\n",
        "plt.ylim(-12,5)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWuTqpn5JXz4"
      },
      "source": [
        "### Lasso Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuL2XpNlJgmw"
      },
      "source": [
        "'''\n",
        "-------- [최종 출력 결과] --------\n",
        "Training-datasset R2 :\n",
        "Test-datasset R2 :\n",
        "Lasso Regression Coefficients :\n",
        "----------------------------------\n",
        "'''\n",
        "# 필요한 라이브러리 로딩\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "# sklearn.linear_model 모듈의 Lasso 클래스 로딩\n",
        "from sklearn.linear_model import Lasso as lasso\n",
        "# 데이터셋 로딩\n",
        "boston = load_boston()\n",
        "\n",
        "# 데이터셋 분할\n",
        "x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.3, random_state=12)\n",
        "\n",
        "# 규제를 위한 alpha 값 초기화\n",
        "\n",
        "alpha = 0.1\n",
        "\n",
        "# Lasso 클래스 객체 생성\n",
        "lasso = lasso(alpha=alpha)\n",
        "\n",
        "# fit() 을 통한 규제 학습 수행\n",
        "lasso.fit(x_train, y_train)\n",
        "\n",
        "# predict() 를 통한 학습된 모델 기반 예측\n",
        "lasso_pred = lasso.predict(x_test)\n",
        "\n",
        "# score() 를 통해 회귀 모델의 R^2 출력\n",
        "# 학습된 모델에 대한 R^2 계산\n",
        "r2_train = lasso.score(x_train, y_train)\n",
        "r2_test = lasso.score(x_test, y_test)\n",
        "print('Training-datasset R2 : {0:.3f}'.format(r2_train))\n",
        "print('Test-datasset R2 : {0:.3f}'.format(r2_test))\n",
        "\n",
        "# 회귀 계수 저장을 위한 Seriess 객체 생성 및 출력\n",
        "lasso_coef_table = pd.Series(data=np.round(lasso.coef_, 1),\n",
        "                        index=boston.feature_names)\n",
        "print('Lasso Regression Coefficients :')\n",
        "print(lasso_coef_table.sort_values(ascending=False))\n",
        "\n",
        "# 막대그래프 시각화 \n",
        "plt.figure(figsize=(10,5))\n",
        "lasso_coef_table.plot(kind='bar')\n",
        "plt.ylim(-10, 4)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42Gny2OTZIjQ"
      },
      "source": [
        "##분류(Classification)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWS4LnJZZhym"
      },
      "source": [
        "###로지스틱 회귀 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wG5oNELbZrg_"
      },
      "source": [
        "'''\n",
        "-------- [최종 출력 결과] --------\n",
        "Training Data Accuracy: ***\n",
        "Testing Data Accuracy: ***\n",
        "Confusion Matrixs : \n",
        " [[***    ***]\n",
        " [ ***    ***]]\n",
        "Accuracy: ***, Precision: ***, Recall: 0.***\n",
        "----------------------------------\n",
        "'''\n",
        "# 필요한 라이브러리 로딩\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# StandardScaler, train_test_split, LogisticRegression 로딩\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# 분류 모델을 위한 성능 지표 함수 로딩\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "# URL 통해서 캐글의 자전거 대여 수요 데이터셋 다운로드\n",
        "url = 'https://codepresso-online-platform-public.s3.ap-northeast-2.amazonaws.com/learning-resourse/python-machine-learning-20210326/bike-demand.csv'\n",
        "df_bike = pd.read_csv(url)\n",
        "\n",
        "# 독립변수 데이터 생성\n",
        "# temp, atemp, humidity, windspeed\t컬럼 데이터만 저장\n",
        "X_df_bike = df_bike.iloc[:, 5:9]\n",
        "#print(X_df_bike.head(5))\n",
        "\n",
        "# 종속변수 데이터를 위한 파생변수 생성\n",
        "# 총 대여건수(count) 가 500 이상인 경우 1, 미만인 경우 0\n",
        "df_bike['y'] = 1\n",
        "df_bike.loc[df_bike['count'] < 500, 'y'] = 0\n",
        "y = df_bike['y'] \n",
        "\n",
        "# StandardScaler 이용한 스케일링\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_df_bike)\n",
        "result = scaler.transform(X_df_bike)\n",
        "\n",
        "# 스케일된 결과 데이터를 DataFrame 으로 저장\n",
        "X_scaled_bike = pd.DataFrame(data=result, columns=X_df_bike.columns)\n",
        "\n",
        "# 데이터셋 분리\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_scaled_bike, y, test_size=0.3 , random_state=12)\n",
        "\n",
        "# LogisticRegression 모델 객체 생성\n",
        "clf = LogisticRegression()\n",
        "\n",
        "# 훈련 데이터를 통한 학습\n",
        "clf.fit(x_train, y_train)\n",
        "\n",
        "# 학습된 모델에 테스트 데이터셋 이용하여 예측값 생성\n",
        "y_pred = clf.predict(x_test)\n",
        "\n",
        "# score 메소드를 통한 정확도 측정\n",
        "train_score = clf.score(x_train, y_train)\n",
        "test_score = clf.score(x_test, y_test)\n",
        "print('Training Data Accuracy: {:0.3f}'.format(train_score))\n",
        "print('Testing Data Accuracy: {:0.3f}'.format(test_score))\n",
        "\n",
        "# 오차 행렬 생성\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "print('Confusion Matrixs : \\n', confusion)\n",
        "\n",
        "# 정확도, 정밀도, 재현율 계산 \n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "  \n",
        "print('Accuracy: {0:.4f}, Precision: {1:.4f}, Recall: {2:.4f}'\n",
        "      .format(accuracy , precision ,recall))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpM6d1XgdqOT"
      },
      "source": [
        "###로지스틱 회귀 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtOYXdKDd3R7"
      },
      "source": [
        "'''\n",
        "-------- [최종 출력 결과] --------\n",
        "Confusion Matrixs : \n",
        " [[***    ***]\n",
        " [ ***    ***]]\n",
        "Accuracy: ***, Precision: ***, Recall: 0.***\n",
        "----------------------------------\n",
        "'''\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 필요한 데이터셋 로딩\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\n",
        "\n",
        "# 데이터셋 로딩\n",
        "cancer = load_breast_cancer()\n",
        "df_cancer = pd.DataFrame(data=cancer.data, columns=cancer.feature_names)\n",
        "# StandardScaler() 활용한 데이터 스케일링 \n",
        "scaler = StandardScaler()\n",
        "scaler.fit(df_cancer)\n",
        "data_scaled = scaler.transform(df_cancer)\n",
        "\n",
        "# 학습데이터와 테스트 데이터로 분할​\n",
        "x_train, x_test, y_train, y_test = train_test_split(data_scaled, \n",
        "                                                    cancer.target, \n",
        "                                                    test_size=0.3, \n",
        "                                                    random_state=12)\n",
        "\n",
        "# 로지스틱 회귀 분석 모델 생성 및 학습\n",
        "clf = LogisticRegression()\n",
        "clf.fit(x_train, y_train)\n",
        "\n",
        "# 학습된 모델에 테스트 데이터(x_test) 입력하여 예측값 생성\n",
        "y_pred = clf.predict(x_test)\n",
        "\n",
        "# 오차행렬 생성 및 출력\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print('Confusion Matrix')\n",
        "print(confusion)\n",
        "\n",
        "# Accuracy, Precision, Recall 확인\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "  \n",
        "  \n",
        "print('Accuracy: {0:.4f},Precision: {1:.4f},Recall: {2:.4f}'\n",
        "      .format(accuracy , precision ,recall))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSBzymQSdiEy"
      },
      "source": [
        "###ROC 커브와 AUC_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88-wEVehonJW"
      },
      "source": [
        "'''\n",
        "-------- [최종 출력 결과] --------\n",
        "Predicted class probability(1st Data) [0.*** 0.***]\n",
        "Target data index(1st Data) *\n",
        "Sample Threshold Index(n=10): [ * * * * * * * * * *]\n",
        "Sample Threshold Value(n=10):  [ *.** 0.** 0.** 0.** 0.** 0.** 0.** 0.** 0.** 0.**]\n",
        "Sample Threshold FPR(n=10): [ 0.*** 0.*** 0.*** 0.*** 0.*** 0.*** 0.*** 0.*** 0.*** 0.***]\n",
        "Sample Threshold TPR(n=10): [ 0.*** 0.*** 0.*** 0.*** 0.*** 0.*** 0.*** 0.*** 0.*** 0.***]\n",
        "Testing Data AUC: 0.***\n",
        "----------------------------------\n",
        "'''\n",
        "# 필요한 라이브러리 로딩\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# URL 통해서 캐글의 자전거 대여 수요 데이터셋 다운로드\n",
        "url = 'https://codepresso-online-platform-public.s3.ap-northeast-2.amazonaws.com/learning-resourse/python-machine-learning-20210326/bike-demand.csv'\n",
        "df_bike = pd.read_csv(url)\n",
        "\n",
        "# 독립변수 데이터 생성\n",
        "# temp, atemp, humidity, windspeed\t컬럼 데이터만 저장\n",
        "X_df_bike = df_bike.iloc[:, 5:9]\n",
        "# print(X_df_bike.head(5))\n",
        "\n",
        "# 종속변수 데이터를 위한 파생변수 생성\n",
        "# 총 대여건수(count) 가 500 이상인 경우 1, 미만인 경우 0\n",
        "df_bike['y'] = 1\n",
        "df_bike.loc[df_bike['count'] < 500, 'y'] = 0\n",
        "y = df_bike['y'] \n",
        "\n",
        "# StandardScaler 이용한 스케일링\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_df_bike)\n",
        "result = scaler.transform(X_df_bike)\n",
        "\n",
        "# 스케일된 결과 데이터를 DataFrame 으로 저장\n",
        "X_scaled_bike = pd.DataFrame(data=result, \n",
        "                             columns=X_df_bike.columns)\n",
        "\n",
        "# 데이터셋 분리\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_scaled_bike, \n",
        "                                                    y, \n",
        "                                                    test_size=0.3, \n",
        "                                                    random_state=12)\n",
        "\n",
        "# LogisticRegression 모델 객체 생성\n",
        "clf = LogisticRegression()\n",
        "\n",
        "# 훈련 데이터를 통한 학습\n",
        "clf.fit(x_train, y_train)\n",
        "\n",
        "# roc_curve() 메소드 로딩\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "# 학습된 모델에서 클래스 별 확률 예측값 생성\n",
        "predict_prob = clf.predict_proba(x_test)\n",
        "print('Predicted class probability(1st Data)', np.round(predict_prob[0], 3))\n",
        "print('Target data index(1st Data)', y_test.values[0])\n",
        "\n",
        "# roc_curve 메소드를 통한 FPR, TPR 계산 값과 이를 위한 Threshold 값 생성\n",
        "fprs , tprs , thresholds = roc_curve(y_test, predict_prob[:, 1])\n",
        "\n",
        "# FPR, TPR 계산 값 Threshold 값을 10건 만 샘플링\n",
        "thr_index = np.arange(0, thresholds.shape[0], 110)\n",
        "print('Sample Threshold Index(n=10):', thr_index)\n",
        "print('Sample Threshold Value(n=10): ', np.round(thresholds[thr_index], 2))\n",
        "print('Sample Threshold FPR(n=10): ', np.round(fprs[thr_index], 3))\n",
        "print('Sample Threshold TPR(n=10): ', np.round(tprs[thr_index], 3))\n",
        "\n",
        "# ROC Curve를 그리기 위한 함수\n",
        "def roc_curve_plot(y_test , pred_proba_c1):\n",
        "    import matplotlib.pyplot as plt\n",
        "    fprs , tprs , thresholds = roc_curve(y_test ,pred_proba_c1)\n",
        "\n",
        "    plt.plot(fprs , tprs, label='ROC')\n",
        "    plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "    \n",
        "    start, end = plt.xlim()\n",
        "    plt.xticks(np.round(np.arange(start, end, 0.1),2))\n",
        "    plt.xlim(0,1); plt.ylim(0,1)\n",
        "    plt.xlabel('FPR'); plt.ylabel('TPR( Recall )')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        " # roc_curve_plot 함수를 이용하여 ROC Curve 생성\n",
        "roc_curve_plot(y_test, clf.predict_proba(x_test)[:, 1])\n",
        "\n",
        "# roc_auc_score() 메소드 로딩\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# roc_auc_score() 메소드를 통한 AUC 측정\n",
        "roc_score = roc_auc_score(y_test.values, predict_prob[:, 1])\n",
        "print('Testing Data AUC: {:0.3f}'.format(roc_score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrRAynx1rdvZ"
      },
      "source": [
        "## Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-53tu4MuQJp"
      },
      "source": [
        "###Decision Tree 모델 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8YDCZ9Drxo2"
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(cancer.data,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\tcancer.target,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\ttest_size=0.3,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\trandom_state=12)\n",
        "# DecisionTreeClassifier 임포트\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# DecisionTreeClassifier 객체 생성\n",
        "dt = DecisionTreeClassifier(random_state=12)\n",
        "\n",
        "# fit 함수로 Decision Tree 모델 학습\n",
        "dt.fit(x_train, y_train)\n",
        "\n",
        "# 학습 된 Tree의 Depth 확인 - get_depth() 함수 사용\n",
        "print(\"Depth of tree: \", dt.get_depth())\n",
        "# 학습 된 Tree의 리프 노드 개수 확인 - get_n_leaves() 함수 사용\n",
        "print(\"Number of leaves: \", dt.get_n_leaves())\n",
        "\n",
        "# predict 함수로 테스트 데이터 세트 예측\n",
        "y_pred = dt.predict(x_test)\n",
        "print(y_pred[0:3])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "250RpHS1uPRb"
      },
      "source": [
        "###Decision Tree 성능 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjFVESj-tOaV"
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "cancer = load_breast_cancer()\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(cancer.data,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\tcancer.target,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\ttest_size=0.3,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\trandom_state=12)\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=12)\n",
        "dt.fit(x_train, y_train)\n",
        "y_pred = dt.predict(x_test)\n",
        "\n",
        "# 성능 지표 측정 함수 임포트 - accuracy_score, precision_score, recall_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "# accuracy 계산\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "# precision 계산\n",
        "precision = precision_score(y_test, y_pred)\n",
        "# recall 계산\n",
        "recall = recall_score(y_test, y_pred)\n",
        "\n",
        "# 성능 지표 출력\n",
        "print(\"Accuracy: {:.3f}\".format(accuracy))\n",
        "print(\"Precision: {:.3f}\".format(precision))\n",
        "print(\"Recall: {:.3f}\".format(recall))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GFZNLzoun2v"
      },
      "source": [
        "###Decision Tree 모델의 Feature 중요도 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnECUaysvEXp"
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "cancer = load_breast_cancer()\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(cancer.data,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\tcancer.target,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\ttest_size=0.3,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\trandom_state=12)\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=12)\n",
        "dt.fit(x_train, y_train)\n",
        "\n",
        "# 학습 된 모델의 Feature Importance 확인\n",
        "for i in range(0,len(cancer.feature_names)):\n",
        "  print('{0}: {1:.3f}'.format(cancer.feature_names[i], dt.feature_importances_[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8zaSMlmwCiU"
      },
      "source": [
        "###hyperparameter 적용 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hb_SNz9iwMP_"
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "cancer = load_breast_cancer()\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(cancer.data,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\tcancer.target,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\ttest_size=0.3,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\trandom_state=12)\n",
        "\n",
        "# Hyperparameter 설정 없이 학습\n",
        "dt = DecisionTreeClassifier(random_state=12)\n",
        "dt.fit(x_train, y_train)\n",
        "\n",
        "# max depth와 leaf 노드 개수 확인, get_depth(), get_n_leaves() 함수 사용\n",
        "print(\"Max Depth: \", dt.get_depth())\n",
        "print(\"Number of leaves: \", dt.get_n_leaves())\n",
        "\n",
        "# max_depth를 3으로 설정 후 학습\n",
        "dt = DecisionTreeClassifier(max_depth=3, random_state=12)\n",
        "dt.fit(x_train, y_train)\n",
        "\n",
        "# max depth와 leaf 노드 개수 확인, get_depth(), get_n_leaves() 함수 사용\n",
        "print(\"Max Depth: \", dt.get_depth())\n",
        "print(\"Number of leaves: \", dt.get_n_leaves())\n",
        "\n",
        "# max_leaf_nodes를 9으로 설정 후 학습\n",
        "dt = DecisionTreeClassifier(max_leaf_nodes=9, random_state=12)\n",
        "dt.fit(x_train, y_train)\n",
        "\n",
        "# max depth와 leaf 노드 개수 확인, get_depth(), get_n_leaves() 함수 사용\n",
        "print(\"Max Depth: \", dt.get_depth())\n",
        "print(\"Number of leaves: \", dt.get_n_leaves())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my_piQ4MzuJa"
      },
      "source": [
        "###Voting Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KwZCvtJz507"
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "x_train, x_test, y_train, y_test = train_test_split(cancer.data,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\tcancer.target,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\ttest_size=0.3,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\trandom_state=12)\n",
        "\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# LogisticRegression 및 DecisionTreeClassifier 객체 생성\n",
        "lr = LogisticRegression(random_state=12)\n",
        "dt = DecisionTreeClassifier(random_state=12)\n",
        "\n",
        "# VotingClassifier 객체 생성\n",
        "voting = VotingClassifier(estimators=[('LR',lr), ('DT',dt)], voting='soft')\n",
        "\n",
        "# VotingClassifier 학습 및 검증\n",
        "voting.fit(x_train , y_train)\n",
        "pred = voting.predict(x_test)\n",
        "\n",
        "# accuracy_score 호출하여 accuracy 계산 후 출력\n",
        "print('Accuracy: {0:.3f}'.format(accuracy_score(y_test, pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMZ4w8Ra4XMv"
      },
      "source": [
        "###Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuIjsn9N4dtN"
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "x_train, x_test, y_train, y_test = train_test_split(cancer.data,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\tcancer.target,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\ttest_size=0.3,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\trandom_state=12)\n",
        "\n",
        "# RandomForestClassifier 임포트\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# RandomForestClassifier 객체 생성\n",
        "rf = RandomForestClassifier(random_state=12)\n",
        "\n",
        "# RandomForestClassifier 객체 학습 및 검증\n",
        "rf.fit(x_train, y_train)\n",
        "pred = rf.predict(x_test)\n",
        "\n",
        "print('Accuracy: {0:.3f}'.format(accuracy_score(y_test, pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Huc3X7zN5D75"
      },
      "source": [
        "##차원 축소"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdvQOpCn5JnA"
      },
      "source": [
        "###주성분 분석(PCA)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKAvJw8d5M-g"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#iris 데이터셋 로드와 Dict 포맷의 키 확인하기\n",
        "data = load_iris()\n",
        "print(\"iris dataset format and keys\\n\",data.keys())\n",
        "\n",
        "#feature name과 관측값 가져오기\n",
        "iris_data = data['data']\n",
        "iris_cols = data['feature_names']\n",
        "print(\"iris dataset columns\\n\",iris_cols)\n",
        "\n",
        "#column name을 재설정\n",
        "iris_cols=['sep_len', 'sep_wt', 'pet_len', 'pet_wt']\n",
        "\n",
        "#데이터프레임 생성\n",
        "iris_df = pd.DataFrame(data= iris_data, columns= iris_cols)\n",
        "iris_df['target'] = data['target']\n",
        "print(iris_df.head(5))\n",
        "\n",
        "#품종별 데이터 카운팅 체크\n",
        "target_cnt_df = iris_df.groupby(by='target').count()\n",
        "print(target_cnt_df)\n",
        "\n",
        "#PCA 수행\n",
        "#표준화(스케일링)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X_train = iris_df.iloc[:, :4]\n",
        "iris_z_score = StandardScaler().fit_transform(X_train) \n",
        "\n",
        "iris_z_df = pd.DataFrame(data= iris_z_score, columns= iris_cols)\n",
        "print(iris_z_df.head(5))\n",
        "\n",
        "#주성분 분석(Feature 4 --> 2 axes)\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "pca.fit(iris_z_df)\n",
        "\n",
        "#주성분 찾기 : 고유벡터\n",
        "print('PCA Shape:\\n',pca.components_.shape )\n",
        "print('PCA eigenvectors:\\n',pca.components_)\n",
        "\n",
        "#고유벡터에 데이터를 투영시키는 과정이 transform이다.\n",
        "X_pca = pca.transform(iris_z_df)\n",
        "print('PCA Projection result(shape)\\n',X_pca.shape)\n",
        "\n",
        "#각 주성분이 분산을 얼마나 잘 설명하는지를 나타냄\n",
        "import numpy as np\n",
        "print('variance :\\n',pca.explained_variance_ratio_)\n",
        "print('total variance :\\n', np.sum(pca.explained_variance_ratio_))\n",
        "print('\\n')\n",
        "\n",
        "#projection 된 결과를 데이터프레임으로 구성\n",
        "pca_cols = ['pca_com_1', 'pca_com_2']\n",
        "pca_df = pd.DataFrame(data= X_pca, columns= pca_cols)\n",
        "pca_df['target'] = data['target']\n",
        "print(pca_df.head(5))\n",
        "\n",
        "#주성분 분석 결과 시각화\n",
        "fig, ax = plt.subplots(ncols=2)\n",
        "\n",
        "sns.scatterplot(iris_df['sep_len'], iris_df['sep_wt'], \n",
        "                hue=iris_df['target'], ax=ax[0])\n",
        "sns.scatterplot(pca_df['pca_com_1'], pca_df['pca_com_2'], \n",
        "                hue=pca_df['target'], ax=ax[1])\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIpz5g2_8mJv"
      },
      "source": [
        "###선형판별분석(LDA)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNOgUceP8qu-"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#iris 데이터셋 로드와 Dict 포맷의 키 확인하기\n",
        "data = load_iris()\n",
        "print(\"iris dataset format and keys\\n\",data.keys())\n",
        "\n",
        "#feature name과 관측값 가져오기\n",
        "iris_data = data['data']\n",
        "iris_cols = data['feature_names']\n",
        "print(\"iris dataset columns\\n\",iris_cols)\n",
        "\n",
        "#column name을 재설정\n",
        "iris_cols=['sep_len', 'sep_wt', 'pet_len', 'pet_wt']\n",
        "\n",
        "#데이터프레임 생성하기 ==> 학습시키기 위한 데이터(독립변수들)\n",
        "iris_df = pd.DataFrame(data= iris_data, columns= iris_cols)\n",
        "print(iris_df.head(5))\n",
        "\n",
        "#데이터프레임에 학습 데이터의 정답값(라벨) 데이터 추가(종속변수)\n",
        "iris_df['label'] = data['target']\n",
        "print(iris_df.head(5))\n",
        "\n",
        "#종속변수 각 그룹에 대해 데이터 카운팅 해보기\n",
        "check_df = iris_df.groupby(by='label').count()\n",
        "print(check_df)\n",
        "\n",
        "#LDA 수행\n",
        "#LDA 패키지 import\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "\n",
        "#학습시키기 위한 독립변수와 종속변수 분할하기\n",
        "X_train = iris_df[iris_cols]\n",
        "y_train = iris_df['label']\n",
        "\n",
        "#LDA 오브젝트생성 및 독립변수와 종속변수를 이용해 LDA 환경구성\n",
        "lda = LDA().fit(X_train, y_train)\n",
        "\n",
        "print(\"판별식 선형계수\\n\",lda.coef_)\n",
        "print(\"판별식 상수\\n\",lda.intercept_)\n",
        "y_pred = pd.DataFrame(lda.predict(X_train))\n",
        "print(\"예측결과\\n\", y_pred.head(5))\n",
        "y_pred_score = pd.DataFrame(lda.predict_proba(X_train))\n",
        "print(\"예측스코어\\n\", y_pred_score.head(5))\n",
        "print(\"예측정확도\\n\", lda.score(X_train,y_train))\n",
        "\n",
        "# 분류 결과 확인하기\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "conf_df = pd.DataFrame(confusion_matrix(y_train, lda.predict(X_train)))\n",
        "conf_df.columns=['pred 0', 'pred 1', 'pred 2']#setosa,versicolor,virginica\n",
        "conf_df.index = ['real 0', 'real 1', 'real 2']\n",
        "print('Confusion Matrix \\n',conf_df)  \n",
        "\n",
        "#시각화로 확인해보기\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "cld=LinearDiscriminantAnalysis()\n",
        "\n",
        "X_lda = cld.fit_transform(X_train, y_train)\n",
        "print(X_lda.shape)\n",
        "\n",
        "#데이터셋 시각화 해보기\n",
        "fig, ax = plt.subplots(ncols=2)\n",
        "\n",
        "sns.scatterplot(iris_df['sep_len'], iris_df['sep_wt'], \n",
        "                hue=iris_df['label'], ax=ax[0])\n",
        "sns.scatterplot(X_lda[:,0], X_lda[:,1], hue=y_train, ax=ax[1])\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoT6XmXVEnTr"
      },
      "source": [
        "##머신러닝 모델 선택"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZj97X9tEsnT"
      },
      "source": [
        "###K-fold 교차 검증 (K-fold Cross Validation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mogiVPekExAZ"
      },
      "source": [
        "'''\n",
        "-------- [최종 출력 결과] --------\n",
        "Fold val accuracy: [0.**, 0.**, 0.**]\n",
        "Avg val accuracy: 0.****\n",
        "----------------------------------\n",
        "'''\n",
        "\n",
        "# K Fold Validation을 위한 cross_val_score() 메서드 로딩\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# 모델 구현을 위한 라이브러리 로딩\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "import numpy as np\n",
        "\n",
        "# load_iris() 메서드를 이용하여 iris 데이터 셋 로드\n",
        "iris = load_iris()\n",
        "data = iris.data\n",
        "label = iris.target\n",
        "\n",
        "# DecisionTreeClassifier 모델 객체 생성\n",
        "dt_clf = DecisionTreeClassifier(random_state=156)\n",
        "\n",
        "# cross_val_score() 메서드를 이용하여 교차 검증 수행\n",
        "scores = cross_val_score(estimator=dt_clf, X=data, y=label, scoring='accuracy', cv=3)\n",
        "\n",
        "# 교차 검증 수행 결과 성능 지표 출력\n",
        "print('Fold val accuracy:',np.round(scores, 4))\n",
        "print('Avg val accuracy:', np.round(np.mean(scores), 4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvTOkblrIYW7"
      },
      "source": [
        "###Grid Search CV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt8hrbihIgkn"
      },
      "source": [
        "'''\n",
        "\n",
        "-------- [최종 출력 결과] --------\n",
        "Optimal parameter: {'max_depth': *, 'min_samples_split': *}\n",
        "Max accuracy: 0.****\n",
        "Test accuracy: 0.****\n",
        "----------------------------------\n",
        "'''\n",
        "\n",
        "# 하이퍼파라미터 튜닝을 을 위한 GridSearchCV 라이브러리 로딩\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# 모델 구현을 위한 라이브러리 로딩\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# load_iris() 메서드를 이용하여 iris 데이터 셋 로드\n",
        "iris = load_iris()\n",
        "\n",
        "# 학습, 테스트 데이터셋 분리\n",
        "x_train, x_test, y_train, y_test = train_test_split(iris.data, \n",
        "                                                    iris.target, \n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=121)\n",
        "\n",
        "\n",
        "# DecisionTreeClassifier 모델 객체 생성\n",
        "dtree = DecisionTreeClassifier()\n",
        "\n",
        "# 모델의 후보 파라미터 셋(param_grid)을 지정한 딕셔너리 객체 생성\n",
        "parameters = {'max_depth':[1,2,3], 'min_sample_split':[2,3]}\n",
        "\n",
        "# GridSearchCV 객체 생성\n",
        "grid_dtree = GridSearchCV(estimator=dtree, param_grid=parameters, cv=3, refit=True)\n",
        "\n",
        "# GridSearchCV 객체의 fit() 메서드를 이용하여\n",
        "# 후보 파라미터 셋의 성능 검증\n",
        "grid_dtree.fit(x_train, y_train)\n",
        "# 후보 파라미터 셋의 성능 검증 결과 출력\n",
        "print('Optimal parameter:', grid_dtree.best_params_)\n",
        "print('Max accuracy: {0:.4f}'.format(grid_dtree.best_score_))\n",
        "\n",
        "# 최적의 파라미터 모델을 이용하여 예측값 생성\n",
        "estimator = grid_dtree.best_estimator_\n",
        "pred = estimator.predict(x_test)\n",
        "\n",
        "# 최적의 파라미터 모델의 성능지표 출력\n",
        "print('Test accuracy: {0:.4f}'.format(accuracy_score(y_test,pred))) \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}