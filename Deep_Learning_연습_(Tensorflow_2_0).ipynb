{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning  연습 (Tensorflow 2.0).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PGiibWCZ3lmo",
        "0D11ALOJIbz5",
        "6olaftwF2E-l",
        "1GSHGX-AvMgl",
        "VLc9O2kkBhf7",
        "jBvdC9m3FMO2",
        "zyZm0XbbvyZG",
        "kHhx4vETVVDo",
        "n8PqjTM1RvVQ",
        "-CLBsN9Q1bTJ",
        "pyr3HBxp1OQK",
        "9mxzVYz88B5S",
        "jo8v_6BgOiqy",
        "WCiuc0QfvpHI",
        "8EFbYRr6KJEb",
        "w5B3VLoeg74L",
        "gAHVy_SkmAPb",
        "Mnq2Vti0pCXD"
      ],
      "authorship_tag": "ABX9TyN0ZuS1vyx6Aw2H3adYO8e4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shinjangwoon/jangwoonshin/blob/practice/Deep_Learning_%EC%97%B0%EC%8A%B5_(Tensorflow_2_0).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxd4J8jS3bF-"
      },
      "source": [
        "#TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGiibWCZ3lmo"
      },
      "source": [
        "##Numpy를 이용한 인공신경망 데이터 이해"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXOxgY8P3uc-"
      },
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\"\"\"\n",
        "1. numpy 라이브러리를 이용하여 아래 속성 정보를 가지는 스칼라 텐서 객체 생성\n",
        "-------- [출력 결과] --------\n",
        "*******1. 스칼라(0D 텐서)*******\n",
        "차원        :  0\n",
        "크기        :  ()\n",
        "데이터 타입 :  int64\n",
        "--------------------------\n",
        "\"\"\"\n",
        "d0_tensor = np.array(3)\n",
        "\n",
        "# 생성한 스칼라 텐서 객체 속성 정보 출력\n",
        "print('*******1. 스칼라(0D 텐서)*******')\n",
        "print('차원        : ', d0_tensor.ndim)\n",
        "print('크기        : ', d0_tensor.shape)\n",
        "print('데이터 타입 : ', d0_tensor.dtype)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "2. numpy 라이브러리를 이용하여 아래 속성 정보를 가지는 백터 텐서 객체 생성\n",
        "-------- [출력 결과] --------\n",
        "*******2. 벡터(1D 텐서)*******\n",
        "차원        :  1\n",
        "크기        :  (5,)\n",
        "데이터 타입 :  float64\n",
        "--------------------------\n",
        "\"\"\"\n",
        "d1_tensor = np.array([1.,2.,3.,4.,5.])\n",
        "\n",
        "# 생성한 벡터 텐서 객체 속성 정보 출력\n",
        "print('\\n*******2. 벡터(1D 텐서)*******')\n",
        "print('차원        : ', d1_tensor.ndim)\n",
        "print('크기        : ', d1_tensor.shape)\n",
        "print('데이터 타입 : ', d1_tensor.dtype)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "3. numpy 라이브러리를 이용하여 아래 속성 정보를 가지는 행렬 텐서 객체 생성\n",
        "-------- [출력 결과] --------\n",
        "*******3. 행렬(2D 텐서)*******\n",
        "차원        :  2\n",
        "크기        :  (3, 2)\n",
        "데이터 타입 :  int64\n",
        "--------------------------\n",
        "\"\"\"\n",
        "d2_tensor = np.array([[1,2],\n",
        "                     [4,5],\n",
        "                      [6,7]])\n",
        "\n",
        "# 생성한 행렬 텐서 객체 속성 정보 출력\n",
        "print('\\n*******3. 행렬(2D 텐서)*******')\n",
        "print('차원        : ', d2_tensor.ndim)\n",
        "print('크기        : ', d2_tensor.shape)\n",
        "print('데이터 타입 : ', d2_tensor.dtype)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "4. numpy 라이브러리를 이용하여 아래 속성 정보를 가지는 3D 텐서 객체 생성\n",
        "-------- [출력 결과] --------\n",
        "*******4. 3D 텐서*******\n",
        "차원        :  3\n",
        "크기        :  (2, 3, 1)\n",
        "데이터 타입 :  int64\n",
        "--------------------------\n",
        "\"\"\"\n",
        "d3_tensor = np.array([[[1],\n",
        "                       [2],\n",
        "                       [3]],\n",
        "                      [[-1],\n",
        "                       [-2],\n",
        "                      [-3]]])\n",
        "# 생성한 3D 텐서 객체 속성 정보 출력\n",
        "print('\\n*******4. 3D 텐서*******')\n",
        "print('차원        : ', d3_tensor.ndim)\n",
        "print('크기        : ', d3_tensor.shape)\n",
        "print('데이터 타입 : ', d3_tensor.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D11ALOJIbz5"
      },
      "source": [
        "##Tensorflow 2.0을 이용한 인공신경망 데이터 이해"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNrjhVGSIpyx"
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\"\"\"\n",
        "1. tensorflow 라이브러리를 이용하여 아래 속성 정보를 가지는 상수 텐서 객체 생성 및 속성 정보 출력\n",
        "-------- [출력 결과] --------\n",
        "*******1. 상수 텐서 객체*******\n",
        "차원        :  3\n",
        "크기        :  (3, 1, 2)\n",
        "데이터 타입 :  <dtype: 'int32'>\n",
        "--------------------------\n",
        "\"\"\"\n",
        "\n",
        "# 상수 텐서 객체 생성\n",
        "d3_tensor = tf.constant([[[1,2]],\n",
        "                         [[3,4]],\n",
        "                         [[5,6]]])\n",
        "\n",
        "# 생성한 상수 텐서 객체 속성 정보 출력\n",
        "print('*******1. 상수 텐서 객체*******')\n",
        "print('차원        : ', d3_tensor.ndim)\n",
        "print('크기        : ', d3_tensor.shape)\n",
        "print('데이터 타입 : ', d3_tensor.dtype)\n",
        "\n",
        "\"\"\"\n",
        "2. 상수 텐서 객체의 값(ndarray) 복사하여 속성 정보 출력\n",
        "-------- [출력 결과] --------\n",
        "*******2. ndarray 객체*******\n",
        "차원        :  3\n",
        "크기        :  (3, 1, 2)\n",
        "데이터 타입 :  int32\n",
        "--------------------------\n",
        "\"\"\"\n",
        "\n",
        "# 상수 텐서에서 .numpy() 메서드 이용하여 numpy ndarray 객체 복사\n",
        "np_d3_tensor = d3_tensor.numpy()\n",
        "\n",
        "np_d3_tensor[0,0,0] = 0\n",
        "print(np_d3_tensor)\n",
        "# 복사한 ndarray 객체의 속성 정보 출력\n",
        "print('\\n*******2. ndarray 객체*******')\n",
        "print('차원        : ', np_d3_tensor.ndim)\n",
        "print('크기        : ', np_d3_tensor.shape)\n",
        "print('데이터 타입 : ', np_d3_tensor.dtype)\n",
        "\n",
        "\"\"\"\n",
        "3. tensorflow 라이브러리를 이용하여 아래 속성 정보를 가지는 변수 텐서 객체 생성 및 속성 정보 출력\n",
        "-------- [출력 결과] --------\n",
        "*******3. 변수 텐서 객체*******\n",
        "크기        :  (2, 3, 1)\n",
        "데이터 타입 :  <dtype: 'int32'>\n",
        "--------------------------\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# 형태의 변수 텐서 객체 생성\n",
        "d3_var_tensor = tf.constant([[[1],[2],[3]],\n",
        "                             [[4],[5],[6]]])\n",
        "\n",
        "\n",
        "print('\\n*******3. 변수 텐서 객체*******')\n",
        "print('크기        : ', d3_var_tensor.shape)\n",
        "print('데이터 타입 : ', d3_var_tensor.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6olaftwF2E-l"
      },
      "source": [
        "##연산자 이해"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uU5BuggO2HDb",
        "outputId": "e11951ea-af5e-4d75-dd8d-4c284bd94864"
      },
      "source": [
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# (2, 3) 형태의 상수 텐서 'tensor_a' 생성\n",
        "tensor_a = tf.constant([[1., 2., 3.],\n",
        "                        [4., 5., 6.]])\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "1. tensorflow 연산자를 이용해 'tensor_a' 텐서에 지수 연산을 적용\n",
        "-------- [출력 결과] --------\n",
        "*******1. 지수연산*******\n",
        "[[ *.** *.** *.** ]\n",
        " [ *.** *.** *.** ]]\n",
        "--------------------------\n",
        "\"\"\"\n",
        "\n",
        "# tf.math 모듈의 지수(exponential)함수를 이용해 'tensor_a' 텐서에 지수 연산 적용\n",
        "print('*******1. 지수연산*******')\n",
        "\n",
        "tensor_exp = tf.math.exp(tensor_a)\n",
        "print(np.round(tensor_exp, 2))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "2. tensorflow 연산자를 이용해 'tensor_a'의 axis 0 을 기준으로 최대값 선택\n",
        "-------- [출력 결과] --------\n",
        "*******2. axis 0 기준 최대값*******\n",
        "[ * * ]\n",
        "--------------------------\n",
        "\"\"\"\n",
        "\n",
        "# tf.math 모듈의 함수를 이용해 'tensor_a'의 axis 0 기준 최대값 선택\n",
        "print('\\n*******2. axis 0 기준 최대값*******')\n",
        "\n",
        "tensor_axis_max = tf.math.reduce_max(tensor_a, axis=0)\n",
        "print(tensor_axis_max)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "3. tensorflow 연산자를 이용해 z = cos(x+y) 계산식에서 x 값이 z 값에 미치는 영향도(미분) 계산\n",
        "-------- [출력 결과] --------\n",
        "*******3. 'tensor_x' 값이 'tensor_z' 값에 미치는 영향도(미분 값)*******\n",
        "[[*.**  *.**  *.**]\n",
        " [*.**  *.**  *.**]]\n",
        "--------------------------\n",
        "\"\"\"\n",
        "\n",
        "tensor_x = tf.Variable([[1., 2., 3.],\n",
        "                        [4., 5., 6.]])\n",
        "\n",
        "tensor_y = tf.Variable([[7., 8., 9.],\n",
        "                        [10., 11., 12.]])\n",
        "\n",
        "print(\"\\n*******3. 'tensor_x' 값이 'tensor_z' 값에 미치는 영향도(미분 값)*******\")\n",
        "with tf.GradientTape() as tape:\n",
        "  # 텐서플로 연산자와 변수 텐서를 이용해 계산그래프 정의\n",
        "  tensor_z = tf.square(tensor_x + tensor_y)\n",
        "  \n",
        "  # tape.gradient를 통해 tensor_x 가 tensor_z 에 미치는 영향도(미분 값)를 계산\n",
        "  gradient_zx = tape.gradient(target=tensor_z, sources=tensor_x)\n",
        "  \n",
        "  # 계산된 영향도(미분 값) 확인\n",
        "  print(np.round(gradient_zx, 2))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*******1. 지수연산*******\n",
            "[[  2.72   7.39  20.09]\n",
            " [ 54.6  148.41 403.43]]\n",
            "\n",
            "*******2. axis 0 기준 최대값*******\n",
            "tf.Tensor([4. 5. 6.], shape=(3,), dtype=float32)\n",
            "\n",
            "*******3. 'tensor_x' 값이 'tensor_z' 값에 미치는 영향도(미분 값)*******\n",
            "[[16. 20. 24.]\n",
            " [28. 32. 36.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GSHGX-AvMgl"
      },
      "source": [
        "##퍼셉트론 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "At4OXlg2vQS9"
      },
      "source": [
        "'''\n",
        "\n",
        "-------- [최종 출력 결과] --------\n",
        "*******모델의 예측 결과*******\n",
        "tf.Tensor(\n",
        "[[ *]\n",
        " [ *]\n",
        " [ *]\n",
        " [ *]], shape=(4, 1), dtype=bool)\n",
        "----------------------------------\n",
        "'''\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 파라미터 값(input 특성 개수, 퍼셉트론의 개수, 학습률) 설정\n",
        "input_dim = 2\n",
        "hidden_units = 1\n",
        "learning_rate = 0.01\n",
        "\n",
        "# 가중치(input 특성 : 2/ 퍼셉트론 : 1)\n",
        "w = tf.Variable(tf.random.uniform(shape=(input_dim, hidden_units)))\n",
        "\n",
        "# 편향(퍼셉트론 : 1)\n",
        "b = tf.Variable(tf.zeros(shape=(hidden_units,)))\n",
        "\n",
        "# 설계한 퍼셉트론 모델의 파라미터 확인(코드 제출시 주석 처리)\n",
        "print('*******퍼셉트론 모델의 초기 가중치*******')\n",
        "print(w)\n",
        "print('\\n*******퍼셉트론 모델의 초기 편향*******')\n",
        "print(b)\n",
        "\n",
        "# 퍼셉트론의 수학 모델 f(x*w + b) 구현\n",
        "def predict(input):\n",
        "  # x*w + b 연산 구현\n",
        "  x = tf.matmul(input, w) + b\n",
        "  # relu 활성화 함수 구현\n",
        "  x = tf.maximum(0, x)\n",
        "  return x\n",
        "\n",
        "# loss(mse)\n",
        "def mse_loss(labels, predictions):\n",
        "  # MSE(Mean Square Error) 연산 구현\n",
        "  loss = tf.reduce_mean(tf.square(labels - predictions))\n",
        "  return loss\n",
        "\n",
        "# train\n",
        "def train(inputs, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    # 퍼셉트론 모델을 예측값을 계산\n",
        "    predictions = predict(inputs)\n",
        "    # 모델의 예측값과 정답간의 에러를 loss 을 이용해 계산\n",
        "    loss = mse_loss(labels, predictions)\n",
        "    # 모델의 파라미터(w, b)가 loss 값에 미치는 영향도를 미분(오차역전파)을 통해 계산\n",
        "    gradient_lw, gradient_lb = tape.gradient(loss, [w, b])\n",
        "  # 경사하강법을 수행하여 모델의 파라미터(w, b) 업데이트\n",
        "  w.assign(w - learning_rate * gradient_lw)\n",
        "  b.assign(b - learning_rate * gradient_lb)\n",
        "  return loss\n",
        "\n",
        "# 퍼셉트론 모델 학습을 위한 OR Gate 데이터 생성\n",
        "inputs = np.array([[0, 0],\n",
        "                   [0, 1],\n",
        "                   [1, 0],\n",
        "                   [1, 1]], dtype=np.float32)\n",
        "\n",
        "labels = np.array([0, 1, 1, 1], dtype=np.float32)\n",
        "\n",
        "# OR Gate 데이터를 차트로 확인(코드 제출시 주석 처리)\n",
        "plt.scatter(inputs[:, 0], inputs[:, 1], c=labels[:])\n",
        "plt.show()\n",
        "\n",
        "# train 함수를 반복적으로 실행하여 퍼셉트론 모델을 학습\n",
        "for epoch in range(100):\n",
        "  # input 데이터와 label 데이터를 한 건씩 추출하여 train 함수에 전달\n",
        "  for x, y in zip(inputs, labels):\n",
        "    loss = train([x], [y])\n",
        "  # 학습 중간에 loss 값의 변화 출력\n",
        "  if (epoch+1)%10 == 0:\n",
        "    print(\"Epoch {}: loss={}\".format(epoch+1, float(loss)))\n",
        "\n",
        "# 학습된 모델에 input 데이터 입력하여 예측결과 계산\n",
        "predictions = predict(inputs)\n",
        "\n",
        "# 모델의 예측결과를 차트로 확인\n",
        "plt.scatter(inputs[:, 0], inputs[:, 1], c=predictions[:]> 0.5)\n",
        "plt.show()\n",
        "\n",
        "print('*******모델의 예측 결과*******')\n",
        "print(predictions[:]> 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLc9O2kkBhf7"
      },
      "source": [
        "##Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQQT7Ip2B0HC"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "#입력 텐서와 타깃 텐서로 이루어진 훈련 데이터를 정의\n",
        "input_tensor = np.random.rand(100, 50)\n",
        "target_tensor = np.random.randint(low=0, high=10, size=100)\n",
        "\n",
        "#입력과 타깃을 매핑하는 층으로 이루어진 네트워크(모델)을 정의\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(units=32, activation='relu', input_shape=(50, )))\n",
        "model.add(layers.Dense(units=10, activation= 'softmax'))\n",
        "\n",
        "#손실 함수, 옵티마이저, 모니터링 하기 위한 성능지표 선택하여 학습 과정을 설정\n",
        "from tensorflow.keras import optimizers\n",
        "model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss='mae', metrics=['accuracy'])\n",
        "\n",
        "#훈련 데이터에 대해 모델 객체의 fit() method를 실행\n",
        "model.fit(x=input_tensor, y=target_tensor, batch_size=10, epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBvdC9m3FMO2"
      },
      "source": [
        "###MNIST 데이터 셋 분류"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wOVOKmKGRPp"
      },
      "source": [
        "#DNN(MLP) 모델을 이용한 MNIST 데이터 셋 분류\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# input tensor 와 Target tensor 준비 (훈련데이터)\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "print(train_images.shape)\n",
        "print(train_labels.shape)\n",
        "\n",
        "train_images[0].shape\n",
        "plt.figure()\n",
        "plt.imshow(train_images[0])\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "\n",
        "train_labels[0]\n",
        "\n",
        "#입력데이터의 전처리\n",
        "# (60000, 28, 28) -> (60000, 28*28)\n",
        "train_images = train_images.reshape((60000, 28*28))\n",
        "print(train_images.shape)\n",
        "\n",
        "print(test_images.shape)\n",
        "test_images = test_images.reshape((10000, 28*28))\n",
        "print(test_images.shape)\n",
        "\n",
        "#DNN(MLP) 모델 디자인\n",
        "from tensorflow.keras import models, layers\n",
        "\n",
        "model = models.Sequential()\n",
        "# hidden layer 256 퍼셉트론\n",
        "\n",
        "model.add(layers.Dense(units=256, activation='relu', input_shape=(28*28,)))\n",
        "model.add(layers.Dense(units=10, activation='softmax'))\n",
        "model.summary()\n",
        "\n",
        "#모델의 학습 정보 설정\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#모델에 input, target 데이터 연결 후 학습\n",
        "history = model.fit(x=train_images, y=train_labels,\n",
        "          epochs=30,\n",
        "          batch_size=128,\n",
        "          validation_split=0.2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e0GWv1AsMJf"
      },
      "source": [
        "#학습 과정 시각화 및 테스트\n",
        "acc = history.history['accuracy']\n",
        "val_acc =history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss =history.history['val_loss']\n",
        "\n",
        "acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Al6PXypVsoT_"
      },
      "source": [
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWxZlKYdtkY7"
      },
      "source": [
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCXQx-MPt-jm",
        "outputId": "c1dbeb9f-829b-4257-b66f-0af014b591bf"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(x=test_images, y=test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9610 - accuracy: 0.9744\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHKjMUdWugo6"
      },
      "source": [
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa9iHOOouow-"
      },
      "source": [
        "predict = model.predict(test_images[0].reshape((1, 28*28)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qeABqHNuybx",
        "outputId": "c6a15306-3ea4-4ff4-eda5-ee7d57accdc9"
      },
      "source": [
        "predict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtxNeB9DvOW2",
        "outputId": "910b2c8a-dc52-49e5-c41a-7564788c2e51"
      },
      "source": [
        "print(np.argmax(predict[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En5L_KuQu-s8"
      },
      "source": [
        "plt.figure()\n",
        "plt.imshow(test_images[0].reshape(28,28))\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyZm0XbbvyZG"
      },
      "source": [
        "###**실습**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHhx4vETVVDo"
      },
      "source": [
        "####fashion-mnist 데이터 셋 분류"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knEcH0TsvwyA"
      },
      "source": [
        "\"\"\"## DNN(MLP) 모델을 이용한 MNIST 데이터 셋 분류\n",
        "* 체점 기준 :\n",
        "  - 데이터 셋 : 체점 서버내 테스트 데이터 셋\n",
        "  - 성능 지표 : Accuracy\n",
        "  - PASS 기준 : 80% 이상\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\"\"\"* Step 1. Input tensor 와 Target tensor 준비(훈련데이터)\"\"\"\n",
        "\n",
        "# label 데이터의 각 value 에 해당하는 class name 정보\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "\n",
        "# 1. import 한 fashion_mnist API를 이용하여 fashion_mnist 데이터 셋을 다운로드 \n",
        "(train_feature_images, train_feature_labels), (test_feature_images, test_feature_labels) = fashion_mnist.load_data()\n",
        "\n",
        "\"\"\"* Step 2. 데이터의 전처리 \"\"\"\n",
        "\n",
        "\n",
        "# 1. 3차원 형태(batch, hight, width)의 train, test feature 데이터를 2차원(batch, hight*width)으로 변경 \n",
        "# 2. feature 데이터를 [0, 1] 사이로 scailing을 수행\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "train_feature_images = train_feature_images.reshape((60000, 28*28))/255.\n",
        "scaler.fit(train_feature_images)\n",
        "train_feature_images = scaler.transform(train_feature_images)\n",
        "test_feature_images = test_feature_images.reshape((10000, 28*28))\n",
        "\n",
        "# 1. 1차원 형태의(batch, ) class index 인 train, test label 데이터를\n",
        "#    to_categorical API를 이용하여 one-hot-encoding 수행하여 2차원(batch, class_cnt) 으로 변경\n",
        "\n",
        "train_feature_labels = tf.keras.utils.to_categorical(train_feature_labels, num_classes=10)\n",
        "test_feature_labels = tf.keras.utils.to_categorical(train_feature_labels, num_classes=10)\n",
        "\n",
        "\"\"\"* Step 3. DNN(MLP) 모델 디자인\"\"\"\n",
        "\n",
        "# 1. Sequential API를 이용하여 fashion_mnist 데이터 셋을 분석 하기 위한 MLP 모델을 디자인 \n",
        "from tensorflow.keras import models, layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(units=256, activation='relu', input_shape=(28*28,)))\n",
        "model.add(layers.Dense(units=10, activation='softmax'))\n",
        "model.summary()\n",
        "\"\"\"* Step 4. 모델의 학습 정보 설정\"\"\"\n",
        "\n",
        "# 1. tf.keras.Model 객체의 compile 메서드를 이용하여 학습을 위한 정보들을 설정\n",
        "#   - optimizer\n",
        "#   - loss : categorical_crossentropy 로 설장(label 데이터를 one-hot-encoding 하였기 때문)\n",
        "#   - metrics : 체점 기준인 accuracy 로 설정\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\"\"\"* Step 5. 모델에 input, target 데이터 연결 후 학습\"\"\"\n",
        "\n",
        "# 1. tf.keras.Model 객체의 fit 메서드를 이용하여 모델을 학습\n",
        "#   - fit 메서드의 verbose=2 로 설정 \n",
        "model.fit(x=train_feature_images, y=train_feature_labels,\n",
        "          epochs=30, \n",
        "          batch_size=128, verbose=2, validation_split=0.2)\n",
        "\n",
        "\n",
        "\"\"\"* 모델 제출 \"\"\"\n",
        "\n",
        "model.save('my_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8PqjTM1RvVQ"
      },
      "source": [
        "####보스턴 집값 회귀 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoxihXcWVzMo"
      },
      "source": [
        "## DNN(MLP) 모델을 이용한 보스터 집값 회귀 분석\n",
        "\"\"\"\n",
        "* 체점 기준 :\n",
        "  - 데이터 셋 : 체점 서버내 테스트 데이터 셋\n",
        "  - 성능 지표 : MAE\n",
        "  - PASS 기준 : 3.0 이하\n",
        "  \"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import boston_housing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\"\"\"* Step 1. Inptu tensor 와 Target tensor 준비(훈련데이터)\"\"\"\n",
        "\n",
        "# 1. import 한 boston_housing API를 이용하여 boston_housing 데이터 셋을 다운로드 \n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.boston_housing.load_data(\n",
        "    path='boston_housing.npz', test_split=0.25, seed=113\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"* Step 2. 데이터의 전처리 \"\"\"\n",
        "\n",
        "# 1. train 데이터의 feature 별 평균값, 표준편차 값을 이용하여 정규화 작업을 진행\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"* Step 3. DNN(MLP) 모델 디자인\"\"\"\n",
        "\n",
        "# 1. Sequential API를 이용하여 boston_housing 데이터 셋을 분석 하기 위한 MLP 모델을 디자인 \n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(8, activation='relu',input_shape=(x_train.shape[1:])))\n",
        "model.add(tf.keras.layers.Dense(4, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"* Step 4. 모델의 학습 정보 설정\"\"\"\n",
        "\n",
        "# 1. tf.keras.Model 객체의 compile 메서드를 이용하여 학습을 위한 정보들을 설정\n",
        "#   - optimizer\n",
        "#   - loss\n",
        "#   - metrics : 체점 기준인 MAE 로 설정\n",
        "model.compile(optimizer='rmsprop', loss='MAE', metrics=['MAE'])\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"* Step 5. 모델에 input, target 데이터 연결 후 학습\"\"\"\n",
        "\n",
        "# 1. tf.keras.Model 객체의 fit 메서드를 이용하여 모델을 학습\n",
        "#   - fit 메서드의 verbose=2 로 설정 \n",
        "model.fit(x_train,y_train, batch_size=8, verbose=2, epochs=100, validation_data=(x_test,y_test))\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"* 모델 제출 \"\"\"\n",
        "\n",
        "model.save('my_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6WVRbTESMYE"
      },
      "source": [
        "##CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CLBsN9Q1bTJ"
      },
      "source": [
        "### MNIST 데이터 셋 분류"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcfEv-nfXd_T"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import mnist\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3VmQ_WlgP2S"
      },
      "source": [
        "\n",
        "\n",
        "* input tensor 와 Target tensor 준비 (훈련데이터)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wy7ImYNac6vK"
      },
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLhnn-RVc6KJ",
        "outputId": "1507152a-0dab-4918-da19-637ebcf3c393"
      },
      "source": [
        "print(train_images.shape)\n",
        "print(train_labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(60000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrS__U60c6CJ",
        "outputId": "62118852-17b6-4d32-b31d-ccb75c8d2618"
      },
      "source": [
        "train_images[0]\n",
        "train_labels[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdShwKO8c5-5"
      },
      "source": [
        "plt.figure()\n",
        "plt.imshow(train_images[0].reshape(28,28))\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qVkdLgSgXXj"
      },
      "source": [
        "\n",
        "\n",
        "* 입력데이터의 전처리\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwfcqbmbc58F"
      },
      "source": [
        "train_images = train_images/255.\n",
        "test_images = test_images/255."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcIuW_eKc55p"
      },
      "source": [
        "# MLP input_data ; 1-D tebsor\n",
        "# convolution input_data : 3-D tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJq5fLn-c5WZ",
        "outputId": "1185bb54-e19c-48c8-99df-69b193a9882e"
      },
      "source": [
        "train_images[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgvPEvwkeQmQ"
      },
      "source": [
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-NVZ0KweQaf",
        "outputId": "aaf28cfb-2946-40b5-fb65-819e847a46e0"
      },
      "source": [
        "train_images.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPIZYWtleQXH",
        "outputId": "dab66dd8-ba21-4797-bbd3-ade72ec34bf1"
      },
      "source": [
        "test_images.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCmoWaA-ey8a"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEYBlO4Vey4_",
        "outputId": "8bf26988-317d-4cff-d1bb-8d811eafe119"
      },
      "source": [
        "train_labels[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDz5jI6LeQUi"
      },
      "source": [
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clh8Uoz5faxR",
        "outputId": "5d9aa9b0-825c-41f0-e735-599342345012"
      },
      "source": [
        "train_labels[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV0WGYK4gBug"
      },
      "source": [
        "\n",
        "\n",
        "*   CNN 모델 디자인\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJcSuvC-faq9"
      },
      "source": [
        "from tensorflow.keras import models, layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9N9t4RRfan0"
      },
      "source": [
        "model = models.Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PK4ZEOj6hGjt"
      },
      "source": [
        "# input data : (28, 28, 1)\n",
        "# feture extraction : convalution layer, pooling layer => 3-D tensor\n",
        "# layer.Flatten() => 1-D tensor\n",
        "# classification : fully connected layer - 1-D tensor\n",
        "# output data : (10, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqKb_0j0hGgQ"
      },
      "source": [
        "model.add(layers.Conv2D(filters=32, kernel_size=(3,3),\n",
        "                        activation='relu',\n",
        "                        input_shape=(28,28,1)))\n",
        "model.add(layers.MaxPool2D(pool_size=(2, 2)))\n",
        "model.add(layers.Conv2D(filters=64, kernel_size=(3, 3),\n",
        "                        activation='relu'))\n",
        "model.add(layers.MaxPool2D(pool_size=(2, 2)))\n",
        "model.add(layers.Conv2D(filters=64, kernel_size=(3, 3),\n",
        "                        activation='relu'))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "model.add(layers.Dense(units=64, activation='relu'))\n",
        "model.add(layers.Dense(units=10, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzE50kYchGc0",
        "outputId": "858cdb06-5fb0-498d-857d-366a5b9a5019"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_15 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 3, 3, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                36928     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 93,322\n",
            "Trainable params: 93,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJfURrbxrzUx"
      },
      "source": [
        "\n",
        "*   모델의 학습 정보 설정\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofTwPjhZhGZt"
      },
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tIlLEbXtjqX"
      },
      "source": [
        "*   모델에 Input, target 데이터 연결 후 학습\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amfgokJ2hGWl"
      },
      "source": [
        "history = model.fit(x=train_images, y=train_labels,\n",
        "          epochs=15, batch_size=256, \n",
        "          validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMijelzhuo4C"
      },
      "source": [
        "* 모델의 성능 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtFhPbtYfRKR",
        "outputId": "1b2ba861-8dae-4f23-98f0-35e0eb466916"
      },
      "source": [
        "test_loss, test_accuracy = model.evaluate(x=test_images, y=test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0350 - accuracy: 0.9925\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f86RGLpZvCq-"
      },
      "source": [
        "* Appendix-1 모델의 학습과정 시각화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jc_oQgtDeQLG"
      },
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc =history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss =history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) +1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "1DWvx996vLvO",
        "outputId": "0d2b55f9-af72-447c-df1e-4502bf6534eb"
      },
      "source": [
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "BfXq8jUlvdDq",
        "outputId": "5cb58308-18b9-46a6-c2af-d5ff69f86ab2"
      },
      "source": [
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_5rFVWzwp7J"
      },
      "source": [
        "* Appendix 모델의 예측 결과 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-olUsHjewwpo",
        "outputId": "8b285db7-9b06-4d5b-eaf9-fabc49b43050"
      },
      "source": [
        "predict = model.predict(x=test_images[0].reshape((1, 28, 28, 1)))\n",
        "predict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.5450631e-14, 1.5223938e-12, 6.0941586e-12, 6.3848575e-12,\n",
              "        2.5822480e-18, 3.9745231e-17, 7.4651047e-21, 1.0000000e+00,\n",
              "        6.3998110e-14, 2.3286535e-13]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10js6kXhxIrc"
      },
      "source": [
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gr97OWyLxIkO",
        "outputId": "35132bf0-da37-4700-8efe-4e561f006c00"
      },
      "source": [
        "np.argmax(predict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHtsTr3mxIg7"
      },
      "source": [
        "plt.figure()\n",
        "plt.imshow(test_images[0].reshape(28,28)*255)\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyr3HBxp1OQK"
      },
      "source": [
        "#### 실습 : CNN 모델을 활용한 fashion-mnist 데이터셋 분류"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtxXrBNm1ljF"
      },
      "source": [
        "\"\"\"## CNN 모델을 이용한 FASHION-MNIST 데이터 셋 분류\n",
        "* 체점 기준 :\n",
        "  - 데이터 셋 : 체점 서버내 테스트 데이터 셋\n",
        "  - 성능 지표 : Accuracy\n",
        "  - PASS 기준 : 80% 이상\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "\"\"\"* Step 1. Inptu tensor 와 Target tensor 준비(훈련데이터)\"\"\"\n",
        "\n",
        "# label 데이터의 각 value 에 해당하는 class name 정보\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. import 한 fashion_mnist API를 이용하여 fashion_mnist 데이터 셋을 다운로드 받으세요\n",
        "(train_images, train_labels),(test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "\n",
        "\"\"\"* Step 2. 입력데이터의 전처리 \"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. 3차원 형태(batch, hight, width)의 train, test feature 데이터를 3차원(batch, hight, width, chanel(1))으로 변경 하세요\n",
        "# 2. feature 데이터를 [0, 1] 사이로 scailing을 수행하세요\n",
        "train_images = train_images/255.\n",
        "test_images = test_images/255.\n",
        "\n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. 1차원 형태의(batch, ) class id 인 train, test label 데이터를\n",
        "#    to_categorical API를 이용하여 one-hot-encoding 수행하여 2차원(batch, class_cnt) 으로 변경 하세요\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "\n",
        "\n",
        "\"\"\"* Step3. CNN 모델 디자인\"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. Sequential API를 이용하여 fashion_mnist 데이터 셋을 분석 하기 위한 CNN 모델을 디자인 하세요\n",
        "from tensorflow.keras import models, layers\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(filters=32, kernel_size=(3,3),\n",
        "                        activation='relu',\n",
        "                        input_shape=(28,28,1)))\n",
        "model.add(layers.MaxPool2D(pool_size=(2, 2)))\n",
        "model.add(layers.Conv2D(filters=64, kernel_size=(3, 3),\n",
        "                        activation='relu'))\n",
        "model.add(layers.MaxPool2D(pool_size=(2, 2)))\n",
        "model.add(layers.Conv2D(filters=64, kernel_size=(3, 3),\n",
        "                        activation='relu'))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "model.add(layers.Dense(units=64, activation='relu'))\n",
        "model.add(layers.Dense(units=10, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\"\"\"* Step 4. 모델의 학습 정보 설정\"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. tf.keras.Model 객체의 compile 메서드를 이용하여 학습을 위한 정보들을 설정하세요\n",
        "#   - optimizer\n",
        "#   - loss\n",
        "#   - metrics : 체점 기준인 accuracy 로 설정\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\"\"\"* Step 5. 모델에 input, target 데이터 연결 후 학습\"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. tf.keras.Model 객체의 fit 메서드를 이용하여 모델을 학습하세요\n",
        "#   - fit 메서드의 verbose=2 로 설정 하세요\n",
        "model.fit(x=train_images, y=train_labels,\n",
        "                    epochs=15, verbose=2,\n",
        "                    batch_size=256,\n",
        "                    validation_split=0.2)\n",
        "\n",
        "\n",
        "\"\"\"* 모델 제출 \"\"\"\n",
        "\n",
        "# 학습된 모델을 제출하기 위한 코드 입니다. 수정하지 마세요\n",
        "model.save('my_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mxzVYz88B5S"
      },
      "source": [
        "### CIFAR-10 데이터 셋 분류"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3xy269L9oV2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import cifar10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ljrbGg_8kbs",
        "outputId": "de362901-fbc9-4375-a76c-05af972a94dd"
      },
      "source": [
        "# Input tensor 와 Target tensor 준비 (훈련데이터)\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Hg8N5Dl8lBt"
      },
      "source": [
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog',\n",
        "               'frog', 'horse', 'ship', 'truck']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvCL8qda8k-3",
        "outputId": "b090b073-c3cd-4447-a093-832826102cf4"
      },
      "source": [
        "print(train_images.shape)\n",
        "print(train_labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 32, 32, 3)\n",
            "(50000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "1i0zCnr8-_To",
        "outputId": "b1423f31-d8fa-4b4d-ccd4-e68bf88abd7d"
      },
      "source": [
        "class_names[train_labels[0][0]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frog'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_OPyIq7-_Qa"
      },
      "source": [
        "plt.imshow(train_images[0], cmap=plt.cm.binary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huxWmTyU-_Mk"
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25) :\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "\n",
        "    plt.xlabel(class_names[train_labels[i][0]])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWJv0UgY-_KP"
      },
      "source": [
        "# 입력데이터의 전처리\n",
        "train_images = train_images/255.\n",
        "test_images = test_images/255."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HG-KMO3U-_HF"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ey8QA1dI-_Eh"
      },
      "source": [
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxNm6c1i-_B5"
      },
      "source": [
        "# CNN 모델 디자인\n",
        "# Input_data : (32, 32, 3)\n",
        "# feature_extraction : => 3-D tensor\n",
        "# - conv layer(filter=32, size=(3,3)), pooling layer(max, (2,2))\n",
        "# - conv layer(filter=64, size=(3,3)), pooling layer(max, (2,2))\n",
        "# - conv layer(filter=64, size=(3,3))\n",
        "# flatten : => (1-D tensor)\n",
        "# classification\n",
        "# - fc layer(64)\n",
        "# - fc layer(10, softmax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3BY2WUuF-aY"
      },
      "source": [
        "from tensorflow.keras import models, layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvyMxoISF-XB"
      },
      "source": [
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(filters=32, kernel_size=(3, 3),\n",
        "                        activation='relu',\n",
        "                        input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
        "model.add(layers.Conv2D(filters=64, kernel_size=(3, 3),\n",
        "                        activation='relu'))\n",
        "model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
        "model.add(layers.Conv2D(filters=64, kernel_size=(3, 3),\n",
        "                        activation='relu'))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "model.add(layers.Dense(units=64, activation='relu'))\n",
        "model.add(layers.Dense(units=10, activation='softmax'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLuXXhFxF-Uv"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrOIEUuhF-SN"
      },
      "source": [
        "# 모델의 학습 정보 설정\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4aQb295F-P0"
      },
      "source": [
        "#모델에 input, target 데이터 연결 후 학습\n",
        "history = model.fit(x=train_images, y=train_labels,\n",
        "          epochs=20,\n",
        "          batch_size=256,\n",
        "          validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFB-lW4ALaft",
        "outputId": "58a5549d-3174-4cf3-d006-de50ab3499be"
      },
      "source": [
        "#모델의 성능 평가\n",
        "test_loss, test_accuracy =model.evaluate(test_images, test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 1.1408 - accuracy: 0.6364\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fT2gTnrrLaXC"
      },
      "source": [
        "#Appendix-1 모델의 학습과정 시각화\n",
        "acc = history.history['accuracy']\n",
        "val_acc =history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss =history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) +1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eTKaxG_LaN0"
      },
      "source": [
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSfiIm5FMl7B"
      },
      "source": [
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdZt7ftcMl3i",
        "outputId": "4ab71457-47db-4f6a-c9cd-9e4835d65873"
      },
      "source": [
        "# Appendix-2 모델의 예측 결과 확인\n",
        "predict = model.predict(test_images[0].reshape(1, 32, 32, 3))\n",
        "\n",
        "predict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.7604262e-03, 3.4653731e-03, 9.6065667e-04, 6.5049130e-01,\n",
              "        6.8080590e-05, 3.2242772e-01, 1.2779334e-02, 2.7685055e-05,\n",
              "        3.9933091e-03, 2.6024425e-05]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-KALglcNBvK"
      },
      "source": [
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "9mRfOfjbNBrp",
        "outputId": "15898ca6-7f46-45fc-8ed3-e2f53fea65dc"
      },
      "source": [
        "class_names[np.argmax(predict)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cat'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OY61ymhbNBo7"
      },
      "source": [
        "plt.figure(figsize=(3, 3))\n",
        "plt.imshow(test_images[0])\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jo8v_6BgOiqy"
      },
      "source": [
        "####실습 : CNN 모델을 활용한 CIFAR-10 데이터셋 분류"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEirnBIsOkza",
        "outputId": "6ec9d32c-04a0-4653-d2e0-fdd3c6afffca"
      },
      "source": [
        "\"\"\"## CNN 모델을 이용한 CIFAR-10-CODEPRESSO 데이터 셋 분류\n",
        "* 체점 기준 :\n",
        "  - 데이터 셋 : 체점 서버내 테스트 데이터 셋\n",
        "  - 성능 지표 : Accuracy\n",
        "  - PASS 기준 : 65% 이상\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import requests\n",
        "import io\n",
        "\n",
        "\n",
        "\"\"\"* Step 1. Inptu tensor 와 Target tensor 준비(훈련데이터)\"\"\"\n",
        "\n",
        "# label 데이터의 각 value 에 해당하는 class name 정보\n",
        "cifar_label_name=['apple', 'beaver', 'bottle', 'butterfly', 'castle',\n",
        "                  'clock', 'couch', 'leopard', 'rose', 'shark']\n",
        "\n",
        "# 데이터 다운로드 url 경로\n",
        "data_url = 'https://codepresso-online-platform-public.s3.ap-northeast-2.amazonaws.com/learning-resourse/Tensorflow+2.0+%EB%94%A5%EB%9F%AC%EB%8B%9D+%EC%99%84%EB%B2%BD+%EA%B0%80%EC%9D%B4%EB%93%9C/cifar-10-codepresso.npz'\n",
        "\n",
        "# requests 라이브러리를 이용해 데이터 다운로드\n",
        "response = requests.get(data_url)\n",
        "response.raise_for_status()\n",
        "\n",
        "# 다운로드 받은 데이터를 읽어 들여 Input tensor 와 Target tensor 준비\n",
        "with np.load(io.BytesIO(response.content)) as cifar_10_codepresso_data:\n",
        "  # 학습 이미지 데이터(np.ndarry, shape=(5000, 32, 32, 3))\n",
        "  train_images = cifar_10_codepresso_data['train_images']\n",
        "  # 학습 라벨 데이터(np.ndarry, shape=(5000, ))\n",
        "  train_labels = cifar_10_codepresso_data['train_labels']\n",
        "  \n",
        "  # 테스트 이미지 데이터(np.ndarry, shape=(1000, 32, 32, 3))\n",
        "  test_images = cifar_10_codepresso_data['test_images']\n",
        "  # 테스트 라벨 데이터(np.ndarry, shape=(1000, ))\n",
        "  test_labels = cifar_10_codepresso_data['test_labels']\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"* Step 2. 입력데이터의 전처리 \"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. feature 데이터를 [0, 1] 사이로 scailing을 수행하세요\n",
        "train_images = train_images/255.\n",
        "test_images = test_images/255.\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. 1차원 형태의(batch, ) class id 인 train, test label 데이터를\n",
        "#    to_categorical API를 이용하여 one-hot-encoding 수행하여 2차원(batch, class_cnt) 으로 변경 하세요\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(train_labels)\n",
        "\n",
        "\n",
        "\"\"\"* Step3. CNN 모델 디자인\"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. Sequential API를 이용하여 fashion_mnist 데이터 셋을 분석 하기 위한 CNN 모델을 디자인 하세요\n",
        "#   - 오버피팅 발생 시 classification module에 layers.Dropout 레이어를 추가 하여 성능을 향상시켜 보세요\n",
        "from tensorflow.keras import models, layers\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(filters=64, kernel_size=(3, 3),\n",
        "                        activation='relu',\n",
        "                        input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
        "model.add(layers.Conv2D(filters=128, kernel_size=(3, 3),\n",
        "                        activation='relu'))\n",
        "model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
        "model.add(layers.Conv2D(filters=128, kernel_size=(3, 3),\n",
        "                        activation='relu'))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "model.add(layers.Dense(units=64, activation='relu'))\n",
        "model.add(layers.Dense(units=10, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\"\"\"* Step 4. 모델의 학습 정보 설정\"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. tf.keras.Model 객체의 compile 메서드를 이용하여 학습을 위한 정보들을 설정하세요\n",
        "#   - optimizer\n",
        "#   - loss\n",
        "#   - metrics : 체점 기준인 accuracy 로 설정\n",
        "model.compile(optimizer='rmsprop' ,\n",
        "              loss='categorical_crossentropy' ,\n",
        "              metrics=['accuracy'])\n",
        "#모델의 성능 평가\n",
        "\n",
        "\"\"\"* Step 5. 모델에 input, target 데이터 연결 후 학습\"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. tf.keras.Model 객체의 fit 메서드를 이용하여 모델을 학습하세요\n",
        "#   - fit 메서드의 verbose=2 로 설정 하세요\n",
        "model.fit(x=train_images, y=train_labels,\n",
        "          epochs=20, verbose=2,\n",
        "          batch_size=256,\n",
        "          validation_split=0.2)\n",
        "\n",
        "\n",
        "\"\"\"* 모델 제출 \"\"\"\n",
        "\n",
        "# 학습된 모델을 제출하기 위한 코드 입니다. 수정하지 마세요\n",
        "model.save('my_model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_33 (Conv2D)           (None, 30, 30, 64)        1792      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_22 (MaxPooling (None, 15, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_34 (Conv2D)           (None, 13, 13, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_23 (MaxPooling (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_35 (Conv2D)           (None, 4, 4, 128)         147584    \n",
            "_________________________________________________________________\n",
            "flatten_11 (Flatten)         (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 64)                131136    \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 355,018\n",
            "Trainable params: 355,018\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "16/16 - 2s - loss: 2.2467 - accuracy: 0.1750 - val_loss: 2.0018 - val_accuracy: 0.2650\n",
            "Epoch 2/20\n",
            "16/16 - 0s - loss: 1.9379 - accuracy: 0.3110 - val_loss: 1.8724 - val_accuracy: 0.3240\n",
            "Epoch 3/20\n",
            "16/16 - 0s - loss: 1.8034 - accuracy: 0.3865 - val_loss: 1.7018 - val_accuracy: 0.4000\n",
            "Epoch 4/20\n",
            "16/16 - 0s - loss: 1.7093 - accuracy: 0.4053 - val_loss: 1.7623 - val_accuracy: 0.3810\n",
            "Epoch 5/20\n",
            "16/16 - 0s - loss: 1.6270 - accuracy: 0.4403 - val_loss: 1.5024 - val_accuracy: 0.4830\n",
            "Epoch 6/20\n",
            "16/16 - 0s - loss: 1.5141 - accuracy: 0.4798 - val_loss: 1.4804 - val_accuracy: 0.4590\n",
            "Epoch 7/20\n",
            "16/16 - 0s - loss: 1.4754 - accuracy: 0.4988 - val_loss: 1.3723 - val_accuracy: 0.5490\n",
            "Epoch 8/20\n",
            "16/16 - 0s - loss: 1.3966 - accuracy: 0.5247 - val_loss: 1.3533 - val_accuracy: 0.5390\n",
            "Epoch 9/20\n",
            "16/16 - 0s - loss: 1.3138 - accuracy: 0.5497 - val_loss: 1.4014 - val_accuracy: 0.5370\n",
            "Epoch 10/20\n",
            "16/16 - 0s - loss: 1.2640 - accuracy: 0.5735 - val_loss: 1.5226 - val_accuracy: 0.4970\n",
            "Epoch 11/20\n",
            "16/16 - 0s - loss: 1.2359 - accuracy: 0.5850 - val_loss: 1.2353 - val_accuracy: 0.5870\n",
            "Epoch 12/20\n",
            "16/16 - 0s - loss: 1.1572 - accuracy: 0.6148 - val_loss: 1.6142 - val_accuracy: 0.4810\n",
            "Epoch 13/20\n",
            "16/16 - 0s - loss: 1.1624 - accuracy: 0.6093 - val_loss: 1.1104 - val_accuracy: 0.6450\n",
            "Epoch 14/20\n",
            "16/16 - 0s - loss: 1.0669 - accuracy: 0.6415 - val_loss: 1.2032 - val_accuracy: 0.5960\n",
            "Epoch 15/20\n",
            "16/16 - 0s - loss: 1.0624 - accuracy: 0.6465 - val_loss: 1.1053 - val_accuracy: 0.6240\n",
            "Epoch 16/20\n",
            "16/16 - 0s - loss: 0.9674 - accuracy: 0.6735 - val_loss: 1.2296 - val_accuracy: 0.5790\n",
            "Epoch 17/20\n",
            "16/16 - 0s - loss: 1.0041 - accuracy: 0.6690 - val_loss: 1.0953 - val_accuracy: 0.6350\n",
            "Epoch 18/20\n",
            "16/16 - 0s - loss: 0.9354 - accuracy: 0.6840 - val_loss: 1.0155 - val_accuracy: 0.6690\n",
            "Epoch 19/20\n",
            "16/16 - 0s - loss: 0.8791 - accuracy: 0.7007 - val_loss: 1.0156 - val_accuracy: 0.6770\n",
            "Epoch 20/20\n",
            "16/16 - 0s - loss: 0.8797 - accuracy: 0.7060 - val_loss: 1.0322 - val_accuracy: 0.6560\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCiuc0QfvpHI"
      },
      "source": [
        "###ImageDataGenerator에 대한 이해 및 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nucw7XcTv44J"
      },
      "source": [
        "from tensorflow import keras\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UH2tGTPPkT97"
      },
      "source": [
        "url = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjLU2v3nwesJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "84c041b7-1eb8-4cda-b67e-33e8841aaafb"
      },
      "source": [
        "keras.utils.get_file(fname='cats_and_dogs_filtered.zip', origin=url, extract=True,\n",
        "                     cache_dir='/content')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "68608000/68606236 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/datasets/cats_and_dogs_filtered.zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLIZ3kNJ1UF8"
      },
      "source": [
        "# ImageDataGenerator를 이용해 이미지 파일을 load 하기 위한 경로 지정\n",
        "train_dir = '/content/datasets/cats_and_dogs_filtered/train'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQQ2lfV51UCk",
        "outputId": "8c0e50ee-d6c7-4dc1-9774-df94af735922"
      },
      "source": [
        "#ImageDataGenerator 객체 생성\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator()\n",
        "type(datagen)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.keras.preprocessing.image.ImageDataGenerator"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyZg_yMh1T21",
        "outputId": "23ee8e96-76a5-4047-e122-4e97fb2a5b0a"
      },
      "source": [
        "#ImageDataGenerator.flow_from_directory() 함수 사용\n",
        "train_genarator = datagen.flow_from_directory(\n",
        "    directory=train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    class_mode='binary'\n",
        ")\n",
        "type(train_genarator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.keras.preprocessing.image.DirectoryIterator"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBHfcuB01TlB",
        "outputId": "6a228eb4-2eb9-4781-9bfc-4f02c60a8535"
      },
      "source": [
        "#DirectoryIterator 객체의 속성 및 메서드\n",
        "#.samples : 연결된 경로에서 읽어들일 이미지 파일의 개수\n",
        "train_genarator.samples\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mv5tzCWQ1Thl",
        "outputId": "4fb37481-c4ef-4b51-d682-83e780e4d5e7"
      },
      "source": [
        "#len() : batch 덩어리의 개수 확인\n",
        "len(train_genarator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "63"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgA0JR131Tex"
      },
      "source": [
        "#next() : batch 데이터 호출\n",
        "x, y = next(train_genarator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kq-Sct9u1Tal",
        "outputId": "385ff7b3-49ae-4c69-9054-c114a494bf98"
      },
      "source": [
        "x.shape, y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((32, 150, 150, 3), (32,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0p1Qyv3N76Vy",
        "outputId": "62c0af18-f86f-4aa5-a559-ef8a978853da"
      },
      "source": [
        "#.getitem(idx) : 원하는 index의 batch 데이터 호출\n",
        "x, y = train_genarator.__getitem__(0)\n",
        "x.shape, y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((32, 150, 150, 3), (32,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNNEB5nE76S4",
        "outputId": "6927216f-ab4e-4d6d-e354-473af7ee7c72"
      },
      "source": [
        "x, y = train_genarator.__getitem__(62)\n",
        "x.shape, y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((16, 150, 150, 3), (16,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjwgn11dDuVx"
      },
      "source": [
        "###CNN 모델을 이용한 Cats and Dogs 데이터 셋 분류"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EFbYRr6KJEb"
      },
      "source": [
        "####1. Input tensor 와 Target tensor 준비 (훈련데이터)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAtXv99-D104"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import  keras\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIOEJ9a5kyoT"
      },
      "source": [
        "url = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1THK_9z6EZC3"
      },
      "source": [
        "path_to_zip = keras.utils.get_file(fname='cats_and_dogs_filtered.zip', origin=url, extract=True,\n",
        "                     cache_dir='/content')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbwwA9zQEY_e"
      },
      "source": [
        "# ImageDataGenerator를 이용해 이미지 파일을 load 하기 위한 경로 지정\n",
        "train_dir = '/content/datasets/cats_and_dogs_filtered/train'\n",
        "validation_dir = '/content/datasets/cats_and_dogs_filtered/validation'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQAU7KT3EY8q",
        "outputId": "393648fe-769d-4a65-d2fa-2347215cf280"
      },
      "source": [
        "# ImageDataGenerator 객체 생성 \n",
        "# 객체 생성 시 rescale 인자룰 이용하여 텐서 내 원소의 범위를 [0~255] => [0~1]로 ReScaling 진행\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# 모든 이미지를 1/255로 스케일을 조정합니다\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# .flow_from_directory() 메서드를 이용하여 학습데이터와 검증데이터를 위한 DirectoryIterator 객체 생성\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    directory=train_dir,\n",
        "    target_size=(150,150),\n",
        "    batch_size=20,\n",
        "    shuffle=True,\n",
        "    class_mode='binary'\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qi2t1jI5EY5q",
        "outputId": "d4763324-c24e-4d23-8b46-d962cd3f5c52"
      },
      "source": [
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    directory=validation_dir,\n",
        "    target_size=(150,150),\n",
        "    batch_size=20,\n",
        "    class_mode='binary'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H54lE2cNEX_z",
        "outputId": "97d93e1a-f927-47c1-9fc2-75b05b6cc243"
      },
      "source": [
        "# next() 함수를 이용하여 학습 데이터의 DirectoryIterator에서 배치 사이즈 만큼의 데이터 load 하여 shape 정보 출력\n",
        "x, y = next(train_generator)\n",
        "x.shape, y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((20, 150, 150, 3), (20,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyW5xKC3JzvO",
        "outputId": "a852ad82-790a-4db2-d11d-f7dd17fff05c"
      },
      "source": [
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
              "       1., 1., 0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwkikhJoKY0J"
      },
      "source": [
        "#### CNN 모델 디자인 및 학습 정보 설정\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IRhntg-KnUt"
      },
      "source": [
        "\n",
        "\n",
        "*   CNN 모델 디자인\n",
        "*   CNN 모델의 summary 정보 출력\n",
        "*   모델의 학습 정보 설정\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rI9U7v4oJzry"
      },
      "source": [
        "# input_data : (150, 150, 3)\n",
        "# feature_extraction : => 3-D tensor\n",
        "# - conv layer(32, (3, 3)), pool layer((2, 2))\n",
        "# - conv layer(64, (3, 3)), pool layer((2, 2))\n",
        "# - conv layer(128, (3, 3)), pool layer((2, 2))\n",
        "# - conv layer(128, (3, 3)), pool layer((2, 2))\n",
        "# flatten : 1-D tensorf\n",
        "# classification\n",
        "# - fc layer(512)\n",
        "# - fc layer(1. sigmoid) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3k-P_D2JzoS"
      },
      "source": [
        "from tensorflow.keras import models, layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRgOf0vNJzlG"
      },
      "source": [
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu',\n",
        "                        input_shape=(150, 150, 3)))\n",
        "model.add(layers.MaxPool2D(2, 2))\n",
        "model.add(layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(layers.MaxPool2D(2, 2))\n",
        "model.add(layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(layers.MaxPool2D(2, 2))\n",
        "model.add(layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(layers.MaxPool2D(2, 2))\n",
        "\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "model.add(layers.Dense(units=512, activation='relu'))\n",
        "model.add(layers.Dense(units=1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UICrnVsCJziy",
        "outputId": "d2b7bb99-4cef-4d73-c55b-db12574ba10c"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 148, 148, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 74, 74, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 72, 72, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 36, 36, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 34, 34, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 17, 17, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 15, 15, 128)       147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               3211776   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 3,453,121\n",
            "Trainable params: 3,453,121\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sx_vlbRhAsi"
      },
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy' ,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5B3VLoeg74L"
      },
      "source": [
        "####모델에 데이터 generator 연결 후 학습\n",
        "\n",
        "*   model.fit() 이용하여 데이터 연결 및 학습\n",
        "*   학습 과정은 history 변수에 저장\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gomr1vO6Jzf0"
      },
      "source": [
        "history = model.fit(train_generator,\n",
        "          epochs=20,\n",
        "          steps_per_epoch=len(train_generator),\n",
        "          validation_data=validation_generator,\n",
        "          validation_steps=len(validation_generator))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAHVy_SkmAPb"
      },
      "source": [
        "####학습 과정 시각화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmawbIcBJzdP"
      },
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc =history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss =history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mnq2Vti0pCXD"
      },
      "source": [
        "####테스트 데이터 셋을 통한 모델의 성능 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aBc51Vvn5GY",
        "outputId": "812ad8b4-3c24-49cf-8437-dd56bb3f254e"
      },
      "source": [
        "val_loss, val_accracy = model.evaluate(validation_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50/50 [==============================] - 3s 53ms/step - loss: 5.3409 - accuracy: 0.6980\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tr9wwbbkqUei"
      },
      "source": [
        "#### 실습 : CNN 모델을 활용한 가위-바위-보 데이터셋 분류\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm86IE-QqTmM"
      },
      "source": [
        "\"\"\"## CNN 모델을 이용한 가위-바위-보 데이터셋 분류분석\n",
        "* 체점 기준 :\n",
        "  - 데이터 셋 : 체점 서버내 테스트 데이터 셋\n",
        "  - 성능 지표 : Accuracy\n",
        "  - PASS 기준 : 80% 이상\n",
        "\"\"\"\n",
        "\n",
        "from tensorflow import keras\n",
        "import os\n",
        "\n",
        "\"\"\" Step 1. Input tensor 와 Target tensor 준비(훈련데이터)\n",
        "    Step 1-(1) 가위-바위-보 데이터셋 다운로드\n",
        "\"\"\"\n",
        "\n",
        "train_url = 'https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps.zip'\n",
        "test_url = 'https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps-test-set.zip'\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. tensorflow.keras.utils 모듈의 get_file API를 이용하여 가위-바위-보 학습 데이터 셋 다운로드\n",
        "\n",
        "train_rps = keras.utils.get_file(fname='rps.zip', origin=train_url, extract=True,\n",
        "                     cache_dir='/content')\n",
        "\n",
        "test_rps = keras.utils.get_file(fname='rps-test-set.zip', origin=test_url, extract=True,\n",
        "                     cache_dir='/content')\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. tensorflow.keras.utils 모듈의 get_file API를 이용하여 가위-바위-보 테스트 데이터 셋 다운로드\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" Step 1-(2) ImageDataGenerator를 이용해 이미지 파일을 load 하기 위한 경로 지정\"\"\"\n",
        "\n",
        "# 1. 저장된 학습, 테스트 데이터를 읽어 오기 위한 경로 정보 생성\n",
        "\n",
        "train_dir = os.path.dirname(train_rps) + '/rps'\n",
        "test_dir = os.path.dirname(test_rps) + '/rps-test-set'\n",
        "\n",
        "\"\"\" Step 1-(3) ImageDataGenerator 객체 생성  \n",
        "    - 객체 생성 시 rescale 인자를 이용하여 텐서 내 원소의 범위를 [0 ~ 255] => [0 ~ 1] 로 ReScaling 진행\n",
        "\"\"\"\n",
        "\n",
        "# 1. ImageDataGenerator API 를 이용하여 학습 데이터와 테스트 데이터를 읽어오기 위한 객체 생성\n",
        "#   - feature 데이터를 [0, 1] 사이로 scailing을 수행\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "\"\"\" - flow_from_directory() 메서드를 이용하여 학습데이터와 검증데이터를 위한 DirectoryIterator 객체 생성\"\"\"\n",
        "\n",
        "# 1. ImageDataGenerator 객체의 flow_from_directory 메서드를 이용하여 데이터를 읽어오기 위한 정보 설정\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    directory=train_dir,\n",
        "    target_size=(150,150),\n",
        "    batch_size=30,\n",
        "    shuffle=True,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    directory=test_dir,\n",
        "    target_size=(150,150),\n",
        "    batch_size=30,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "\"\"\" Step 2. CNN 모델 디자인\"\"\"\n",
        "\n",
        "# 1. Sequential API를 이용하여 가위-바위-보 데이터셋 을 분석 하기 위한 CNN 모델을 디자인\n",
        "from tensorflow.keras import models, layers\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu',\n",
        "                        input_shape=(150,150,3)))\n",
        "model.add(layers.MaxPool2D(2,2))\n",
        "model.add(layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu'))\n",
        "model.add(layers.MaxPool2D(2,2))\n",
        "model.add(layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu'))\n",
        "model.add(layers.MaxPool2D(2,2))\n",
        "model.add(layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu'))\n",
        "model.add(layers.MaxPool2D(2,2))\n",
        "\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "model.add(layers.Dense(units=512, activation='relu'))\n",
        "model.add(layers.Dense(units=3, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\"\"\" Step 3. 모델의 학습 정보 설정\"\"\"\n",
        "\n",
        "# 1. tf.keras.Model 객체의 compile 메서드를 이용하여 학습을 위한 정보들을 설정\n",
        "#   - optimizer\n",
        "#   - loss\n",
        "#   - metrics : 체점 기준인 accuracy 로 설정\n",
        "model.compile(optimizer='rmsprop' ,\n",
        "              loss='categorical_crossentropy' ,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\"\"\" Step 4. 모델에 데이터 generator 연결 후 학습\"\"\"\n",
        "\n",
        "# 1. tf.keras.Model 객체의 fit 메서드를 이용하여 모델을 학습\n",
        "#   - fit 메서드의 verbose=2 로 설정 \n",
        "history = model.fit(train_generator,\n",
        "                    steps_per_epoch=len(train_generator),\n",
        "                    epochs=20, verbose=2)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "\n",
        "\"\"\"* 모델 제출 \"\"\"\n",
        "\n",
        "model.save('my_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnhJR3RJmBxy"
      },
      "source": [
        "###DataAugmentation을 통한 Cats & Dogs 분류 분석 모델의 성능 개선\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "2BHKP5Jcpb5k",
        "outputId": "d8813efd-ebf3-43c1-d34f-33cbe4b3cdf5"
      },
      "source": [
        "'''\n",
        " overfitting:\n",
        " - train : 실제 부딪힐수 있는 모든 상황의 데이터를 갖지 못해 발생\n",
        " Augmentation :\n",
        " - train 데이터에 약간의 변형을 가해, 데이터의 분포를 다양하게 만듬\n",
        " '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n overfitting:\\n - train : 실제 부딪힐수 있는 모든 상황의 데이터를 갖지 못해 발생\\n Augmentation :\\n - train 데이터에 약간의 변형을 가해, 데이터의 분포를 다양하게 만듬\\n '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugFbGVx6qLyX"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Juem8ZxRmOPi",
        "outputId": "639ad4c2-93ac-40e0-f333-33cc93da0e5a"
      },
      "source": [
        "#Cats and Dogs 데이터 셋 다운로드\n",
        "url = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
        "path_to_zip = keras.utils.get_file(fname='cats_and_dogs_filtered.zip', origin=url, extract=True,\n",
        "                     cache_dir='/content')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "68608000/68606236 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Iky445HpZds"
      },
      "source": [
        "#ImageDataGenerator를 이용해 이미지 파일을 load 하기 위한 경로 지정\n",
        "train_dir = '/content/datasets/cats_and_dogs_filtered/train'\n",
        "validation_dir = '/content/datasets/cats_and_dogs_filtered/validation'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXQZlTShpZaN",
        "outputId": "a9a50985-5074-4677-e455-eb29dfdd7862"
      },
      "source": [
        "#ImageDataGenerator 객체 생성\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   rotation_range=40,\n",
        "                                   width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True\n",
        ")\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "#flow_from_directory() 메서드를 이용하여 학습데이터와 검증데이터를 위한 DirectoryIterator 객체 생성\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    directory=train_dir,\n",
        "    target_size=(150,150),\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    class_mode='binary'\n",
        ")\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    directory=validation_dir,\n",
        "    target_size=(150,150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYRKu067pZUq"
      },
      "source": [
        "# CNN 모델 디자인 및 학습 정보 설정\n",
        "##모델 구성\n",
        "model = keras.Sequential()\n",
        "\n",
        "model.add(keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu',\n",
        "                        input_shape=(150, 150, 3)))\n",
        "model.add(keras.layers.MaxPool2D(2, 2))\n",
        "model.add(keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(keras.layers.MaxPool2D(2, 2))\n",
        "model.add(keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(keras.layers.MaxPool2D(2, 2))\n",
        "model.add(keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(keras.layers.MaxPool2D(2, 2))\n",
        "\n",
        "\n",
        "model.add(keras.layers.Flatten())\n",
        "\n",
        "model.add(keras.layers.Dense(units=512, activation='relu'))\n",
        "model.add(keras.layers.Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "## 학습을 위한 설정 정보 지정\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy' ,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWrnQ5rlpZRp"
      },
      "source": [
        "history = model.fit(\n",
        "          train_generator,\n",
        "          steps_per_epoch=len(train_generator),\n",
        "          epochs=100,\n",
        "          validation_data=validation_generator,\n",
        "          validation_steps=len(validation_generator))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-J8ig6OwGnd"
      },
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc =history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss =history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNDwalB_wGkE",
        "outputId": "e683f1a5-14d8-4e6a-904d-6d7e6c4774e7"
      },
      "source": [
        "test_loss, test_accuracy = model.evaluate(validation_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 3s 82ms/step - loss: 0.5570 - accuracy: 0.7160\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMfNz8vFBf-w"
      },
      "source": [
        "####실습 : DataAugmentation을 통한 가위-바위-보 분류분석 모델의 성능 개선\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJuGX4wIBigM",
        "outputId": "e8031901-c10a-4512-8053-3626f64159dc"
      },
      "source": [
        "\"\"\"[05-05] DataAugmentation을 통한 가위-바위-보 분류분석 모델의 성능 개선\n",
        "* 체점 기준 :\n",
        "  - 데이터 셋 : 체점 서버내 테스트 데이터 셋\n",
        "  - 성능 지표 : Accuracy\n",
        "  - PASS 기준 : 90% 이상\n",
        "\"\"\"\n",
        "from tensorflow import keras\n",
        "\n",
        "import os\n",
        "\n",
        "\"\"\" Step 1. Input tensor 와 Target tensor 준비(훈련데이터)\n",
        "    Step 1-(1) 가위-바위-보 데이터셋 다운로드\n",
        "\"\"\"\n",
        "\n",
        "train_url = 'https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps.zip'\n",
        "test_url = 'https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps-test-set.zip'\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. tensorflow.keras.utils 모듈의 get_file API를 이용하여 가위-바위-보 학습 데이터 셋 다운로드\n",
        "train_path = keras.utils.get_file(fname='rps.zip', origin=train_url,\n",
        "                                  extract=True, cache_dir='/content')\n",
        "\n",
        "\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. tensorflow.keras.utils 모듈의 get_file API를 이용하여 가위-바위-보 테스트 데이터 셋 다운로드\n",
        "test_path = keras.utils.get_file(fname='rps-test-set.zip', origin=test_url, extract=True,\n",
        "                                  cache_dir='/content')\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" Step 1-(2) ImageDataGenerator를 이용해 이미지 파일을 load 하기 위한 경로 지정\"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. 저장된 학습, 테스트 데이터를 읽어 오기 위한 경로 정보 생성\n",
        "#   - hint : os.path.dirname() 메서드를 이용하여 데이터 셋이 저장된 경로 추출\n",
        "#     => /root/.keras/datasets/rps_test-set.zip => /root/.keras/datasets/\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "train_dir = os.path.dirname(train_path) + '/rps'\n",
        "test_dir = os.path.dirname(test_path) + '/rps-test-set'\n",
        "\n",
        "\n",
        "\"\"\" Step 1-(3) ImageDataGenerator 객체 생성  \n",
        "    - 객체 생성 시 rescale 인자를 이용하여 텐서 내 원소의 범위를 [0 ~ 255] => [0 ~ 1] 로 ReScaling 진행\n",
        "    - Data Augmentation 을 위한 정보들 설정\n",
        "    - 검증 데이터 셋은 Augmentation 미수행\n",
        "\"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. ImageDataGenerator API 를 이용하여 학습 데이터와 테스트 데이터를 읽어오기 위한 객체 생성\n",
        "#   - feature 데이터를 [0, 1] 사이로 scailing을 수행하세요\n",
        "#   - Data Augmentation 을 위한 정보들을 설정하세요\n",
        "#   - 검증 데이터 셋은 Augmentation 미수행 해야 정확학 성능이 측정 됩니다.\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   rotation_range=40,\n",
        "                                   width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True\n",
        ")\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\"\"\" - flow_from_directory() 메서드를 이용하여 학습데이터와 검증데이터를 위한 DirectoryIterator 객체 생성\"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. ImageDataGenerator 객체의 flow_from_directory 메서드를 이용하여 데이터를 읽어오기 위한 정보 설정\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    directory=train_dir,\n",
        "    target_size=(150,150),\n",
        "    batch_size=20,\n",
        "    shuffle=True,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    directory=test_dir,\n",
        "    target_size=(150,150),\n",
        "    batch_size=20,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "\n",
        "\"\"\" Step 2. CNN 모델 디자인\"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. Sequential API를 이용하여 가위-바위-보 데이터셋 을 분석 하기 위한 CNN 모델을 디자인 하세요\n",
        "\n",
        "model = keras.Sequential()\n",
        "\n",
        "model.add(keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu',\n",
        "                        input_shape=(150, 150, 3)))\n",
        "model.add(keras.layers.MaxPool2D(2, 2))\n",
        "model.add(keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(keras.layers.MaxPool2D(2, 2))\n",
        "model.add(keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(keras.layers.MaxPool2D(2, 2))\n",
        "model.add(keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(keras.layers.MaxPool2D(2, 2))\n",
        "\n",
        "\n",
        "model.add(keras.layers.Flatten())\n",
        "\n",
        "model.add(keras.layers.Dense(units=512, activation='relu'))\n",
        "model.add(keras.layers.Dense(units=3, activation='softmax'))\n",
        "\n",
        "\n",
        "\"\"\" Step 3. 모델의 학습 정보 설정\"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. tf.keras.Model 객체의 compile 메서드를 이용하여 학습을 위한 정보들을 설정하세요\n",
        "#   - optimizer\n",
        "#   - loss\n",
        "#   - metrics : 체점 기준인 accuracy 로 설정\n",
        "\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy' ,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\"\"\" Step 4. 모델에 데이터 generator 연결 후 학습\"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. tf.keras.Model 객체의 fit 메서드를 이용하여 모델을 학습하세요\n",
        "#   - fit 메서드의 verbose=2 로 설정 하세요\n",
        "history = model.fit(\n",
        "          train_generator,\n",
        "          steps_per_epoch=len(train_generator),\n",
        "          epochs=20,\n",
        "          validation_data=test_generator,\n",
        "          validation_steps=len(test_generator))\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "\n",
        "\"\"\"* 모델 제출 \"\"\"\n",
        "\n",
        "# 학습된 모델을 제출하기 위한 코드 입니다. 수정하지 마세요\n",
        "model.save('my_model.h5')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2520 images belonging to 3 classes.\n",
            "Found 372 images belonging to 3 classes.\n",
            "Epoch 1/20\n",
            "126/126 [==============================] - 24s 164ms/step - loss: 1.1410 - accuracy: 0.4345 - val_loss: 0.5769 - val_accuracy: 0.9758\n",
            "Epoch 2/20\n",
            "126/126 [==============================] - 20s 162ms/step - loss: 0.6223 - accuracy: 0.7440 - val_loss: 0.1740 - val_accuracy: 0.9785\n",
            "Epoch 3/20\n",
            "126/126 [==============================] - 20s 163ms/step - loss: 0.3496 - accuracy: 0.8706 - val_loss: 0.0757 - val_accuracy: 0.9892\n",
            "Epoch 4/20\n",
            "126/126 [==============================] - 20s 162ms/step - loss: 0.2384 - accuracy: 0.9131 - val_loss: 0.0374 - val_accuracy: 1.0000\n",
            "Epoch 5/20\n",
            "126/126 [==============================] - 21s 163ms/step - loss: 0.1991 - accuracy: 0.9329 - val_loss: 0.0067 - val_accuracy: 1.0000\n",
            "Epoch 6/20\n",
            "126/126 [==============================] - 20s 162ms/step - loss: 0.1493 - accuracy: 0.9528 - val_loss: 0.0965 - val_accuracy: 0.9597\n",
            "Epoch 7/20\n",
            "126/126 [==============================] - 20s 163ms/step - loss: 0.1318 - accuracy: 0.9556 - val_loss: 0.0778 - val_accuracy: 0.9677\n",
            "Epoch 8/20\n",
            "126/126 [==============================] - 20s 162ms/step - loss: 0.1067 - accuracy: 0.9611 - val_loss: 0.0268 - val_accuracy: 0.9839\n",
            "Epoch 9/20\n",
            "126/126 [==============================] - 20s 161ms/step - loss: 0.1176 - accuracy: 0.9627 - val_loss: 0.0395 - val_accuracy: 0.9839\n",
            "Epoch 10/20\n",
            "126/126 [==============================] - 20s 162ms/step - loss: 0.1256 - accuracy: 0.9651 - val_loss: 0.0862 - val_accuracy: 0.9677\n",
            "Epoch 11/20\n",
            "126/126 [==============================] - 21s 163ms/step - loss: 0.1082 - accuracy: 0.9663 - val_loss: 0.0373 - val_accuracy: 0.9731\n",
            "Epoch 12/20\n",
            "126/126 [==============================] - 20s 162ms/step - loss: 0.0913 - accuracy: 0.9706 - val_loss: 0.3056 - val_accuracy: 0.9005\n",
            "Epoch 13/20\n",
            "126/126 [==============================] - 20s 162ms/step - loss: 0.1095 - accuracy: 0.9706 - val_loss: 0.0387 - val_accuracy: 0.9758\n",
            "Epoch 14/20\n",
            "126/126 [==============================] - 20s 163ms/step - loss: 0.0762 - accuracy: 0.9758 - val_loss: 0.1093 - val_accuracy: 0.9677\n",
            "Epoch 15/20\n",
            "126/126 [==============================] - 21s 163ms/step - loss: 0.0948 - accuracy: 0.9782 - val_loss: 0.0137 - val_accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "126/126 [==============================] - 20s 162ms/step - loss: 0.0605 - accuracy: 0.9806 - val_loss: 0.1106 - val_accuracy: 0.9651\n",
            "Epoch 17/20\n",
            "126/126 [==============================] - 20s 162ms/step - loss: 0.0859 - accuracy: 0.9770 - val_loss: 0.0128 - val_accuracy: 0.9946\n",
            "Epoch 18/20\n",
            "126/126 [==============================] - 20s 162ms/step - loss: 0.0820 - accuracy: 0.9770 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "126/126 [==============================] - 21s 163ms/step - loss: 0.0549 - accuracy: 0.9845 - val_loss: 0.0298 - val_accuracy: 0.9839\n",
            "Epoch 20/20\n",
            "126/126 [==============================] - 20s 162ms/step - loss: 0.0728 - accuracy: 0.9813 - val_loss: 0.1014 - val_accuracy: 0.9677\n",
            "19/19 [==============================] - 1s 61ms/step - loss: 0.1014 - accuracy: 0.9677\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVl6Eo3odL3y"
      },
      "source": [
        "###Transfer Learning을 통한 Cats & Dogs 분류 분석 모델의 성능 개선"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJAKzRlSdJOw"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "op6C3Zh2dvMp",
        "outputId": "1bdfedc5-0ad0-4baa-c609-6cbd2b27ab43"
      },
      "source": [
        "#Cats and Dogs 데이터 셋 다운로드\n",
        "url = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
        "path_to_zip = keras.utils.get_file(fname='cats_and_dogs_filtered.zip', origin=url, extract=True,\n",
        "                     cache_dir='/content')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "68608000/68606236 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9iJ5f8edvJN"
      },
      "source": [
        "#ImageDataGenerator를 이용해 이미지 파일을 load 하기 위한 경로 지정\n",
        "train_dir = '/content/datasets/cats_and_dogs_filtered/train'\n",
        "validation_dir = '/content/datasets/cats_and_dogs_filtered/validation'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w16kkk8zdvHK"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ma6-iKmKdvEG",
        "outputId": "a76e6ea4-28d3-48c9-f151-d26f4d530a8c"
      },
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "    directory=train_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=20,\n",
        "    shuffle=True,\n",
        "    class_mode='binary'\n",
        ")\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    directory=validation_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=20,\n",
        "    class_mode='binary'\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRHye24VfZOQ"
      },
      "source": [
        "####VGG16을 Backbone으로 하는 모델 디자인 및 학습 정보 설정\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGnmzE9TdvBu"
      },
      "source": [
        "'''\n",
        "Pre-trained 된 VGG16 모델 객체 생성\n",
        "-imagenet 데이터를 이용해 학습된 모델 객체 생성\n",
        "-classification layer 제외\n",
        "'''\n",
        "from tensorflow.keras.applications import VGG16\n",
        "conv_base = VGG16(weights='imagenet',\n",
        "                  include_top=False,\n",
        "                  input_shape=(224, 224, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8kg0X5Bdu-1",
        "outputId": "eb9b72a0-7fb4-49f3-8915-3a0447db2f94"
      },
      "source": [
        "conv_base.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJCwERkJdu8H"
      },
      "source": [
        "#VGG16 Backbone 모델에 classification layer 추가\n",
        "model = keras.models.Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-iGBZs4du5j"
      },
      "source": [
        "model.add(conv_base)\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dense(units=256, activation='relu'))\n",
        "model.add(keras.layers.Dense(units=1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUctC_OEiFQz",
        "outputId": "77246598-58a3-4d2b-c776-ef7cc93a9957"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vgg16 (Functional)           (None, 7, 7, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               6422784   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 21,137,729\n",
            "Trainable params: 21,137,729\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-6k50-biFNX"
      },
      "source": [
        "#VGG16 Backbone 모델의 가중치 동결(학습대상 가중치에서 제외)\n",
        "conv_base.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyCcIttDiFKe",
        "outputId": "9c75460a-6f1a-411a-ca8a-0660be43134b"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vgg16 (Functional)           (None, 7, 7, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               6422784   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 21,137,729\n",
            "Trainable params: 6,423,041\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4USS5psviFIC"
      },
      "source": [
        "#학습을 위한 설정 정보 지정\n",
        "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=2e-5),\n",
        "              loss='binary_crossentropy' ,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDzepsNWiFF0",
        "outputId": "60b3b571-3cb1-4f89-f384-6a12461ce513"
      },
      "source": [
        "history = model.fit(\n",
        "          train_generator,\n",
        "          steps_per_epoch=len(train_generator),\n",
        "          epochs=30,\n",
        "          validation_data=validation_generator,\n",
        "          validation_steps=len(validation_generator))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "100/100 [==============================] - 53s 187ms/step - loss: 0.4489 - accuracy: 0.7970 - val_loss: 0.3145 - val_accuracy: 0.8870\n",
            "Epoch 2/30\n",
            "100/100 [==============================] - 17s 170ms/step - loss: 0.2560 - accuracy: 0.9010 - val_loss: 0.2975 - val_accuracy: 0.8690\n",
            "Epoch 3/30\n",
            "100/100 [==============================] - 16s 165ms/step - loss: 0.1961 - accuracy: 0.9325 - val_loss: 0.2307 - val_accuracy: 0.9080\n",
            "Epoch 4/30\n",
            "100/100 [==============================] - 16s 164ms/step - loss: 0.1522 - accuracy: 0.9530 - val_loss: 0.2263 - val_accuracy: 0.9040\n",
            "Epoch 5/30\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 0.1230 - accuracy: 0.9605 - val_loss: 0.2120 - val_accuracy: 0.9170\n",
            "Epoch 6/30\n",
            "100/100 [==============================] - 17s 169ms/step - loss: 0.1024 - accuracy: 0.9715 - val_loss: 0.2086 - val_accuracy: 0.9180\n",
            "Epoch 7/30\n",
            "100/100 [==============================] - 17s 169ms/step - loss: 0.0820 - accuracy: 0.9815 - val_loss: 0.2295 - val_accuracy: 0.9000\n",
            "Epoch 8/30\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 0.0682 - accuracy: 0.9845 - val_loss: 0.2192 - val_accuracy: 0.9130\n",
            "Epoch 9/30\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 0.0531 - accuracy: 0.9910 - val_loss: 0.2098 - val_accuracy: 0.9170\n",
            "Epoch 10/30\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 0.0445 - accuracy: 0.9945 - val_loss: 0.2094 - val_accuracy: 0.9170\n",
            "Epoch 11/30\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 0.0377 - accuracy: 0.9955 - val_loss: 0.2250 - val_accuracy: 0.9030\n",
            "Epoch 12/30\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 0.0295 - accuracy: 0.9970 - val_loss: 0.2162 - val_accuracy: 0.9080\n",
            "Epoch 13/30\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 0.0241 - accuracy: 0.9995 - val_loss: 0.2833 - val_accuracy: 0.8870\n",
            "Epoch 14/30\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 0.0194 - accuracy: 0.9990 - val_loss: 0.2209 - val_accuracy: 0.9090\n",
            "Epoch 15/30\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 0.0155 - accuracy: 0.9995 - val_loss: 0.2286 - val_accuracy: 0.9100\n",
            "Epoch 16/30\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.2453 - val_accuracy: 0.9100\n",
            "Epoch 17/30\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.2478 - val_accuracy: 0.9140\n",
            "Epoch 18/30\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.2588 - val_accuracy: 0.9050\n",
            "Epoch 19/30\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.2579 - val_accuracy: 0.9080\n",
            "Epoch 20/30\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.2670 - val_accuracy: 0.9070\n",
            "Epoch 21/30\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.2705 - val_accuracy: 0.9140\n",
            "Epoch 22/30\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.2745 - val_accuracy: 0.9120\n",
            "Epoch 23/30\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.3175 - val_accuracy: 0.9040\n",
            "Epoch 24/30\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.2911 - val_accuracy: 0.9080\n",
            "Epoch 25/30\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.2946 - val_accuracy: 0.9090\n",
            "Epoch 26/30\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.3163 - val_accuracy: 0.9020\n",
            "Epoch 27/30\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 9.6690e-04 - accuracy: 1.0000 - val_loss: 0.3087 - val_accuracy: 0.9070\n",
            "Epoch 28/30\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 8.1017e-04 - accuracy: 1.0000 - val_loss: 0.3175 - val_accuracy: 0.9080\n",
            "Epoch 29/30\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 5.8407e-04 - accuracy: 1.0000 - val_loss: 0.3211 - val_accuracy: 0.9070\n",
            "Epoch 30/30\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 4.5709e-04 - accuracy: 1.0000 - val_loss: 0.3349 - val_accuracy: 0.9070\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "cHmAdU0xiFDS",
        "outputId": "a0a98dc3-a855-4737-a4b0-366a00a90efb"
      },
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc =history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss =history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPv25yg4ykbf",
        "outputId": "97157a1d-4820-488a-bde5-fc5454ffe3a3"
      },
      "source": [
        "test_loss, test_accuracy = model.evaluate(validation_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50/50 [==============================] - 6s 109ms/step - loss: 0.3349 - accuracy: 0.9070\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9OUDJ62dmPA"
      },
      "source": [
        "###Transfer Learning을 통한 가위-바위-보 분류 분석 모델의 성능 개선"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihU8SkcH08GV"
      },
      "source": [
        "\"\"\"[05-05] [05-07] Transfer Learning을 통한 가위-바위-보 분류 분석 모델의 성능 개선\n",
        "* 체점 기준 :\n",
        "  - 데이터 셋 : 체점 서버내 테스트 데이터 셋\n",
        "  - 성능 지표 : Accuracy\n",
        "  - PASS 기준 : 90% 이상\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\"\"\" Step 1. Input tensor 와 Target tensor 준비(훈련데이터)\n",
        "    Step 1-(1) 가위-바위-보 데이터셋 다운로드\n",
        "\"\"\"\n",
        "\n",
        "train_url = 'https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps.zip'\n",
        "test_url = 'https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps-test-set.zip'\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. tensorflow.keras.utils 모듈의 get_file API를 이용하여 가위-바위-보 학습 데이터 셋 다운로드\n",
        "train_path = keras.utils.get_file(fname='rps.zip', origin=train_url,\n",
        "                                  extract=True, cache_dir='/content')\n",
        "\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. tensorflow.keras.utils 모듈의 get_file API를 이용하여 가위-바위-보 테스트 데이터 셋 다운로드\n",
        "test_path = keras.utils.get_file(fname='rps-test-set.zip', origin=test_url, extract=True,\n",
        "                                  cache_dir='/content')\n",
        "\n",
        "\n",
        "\"\"\" Step 1-(2) ImageDataGenerator를 이용해 이미지 파일을 load 하기 위한 경로 지정\"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. 저장된 학습, 테스트 데이터를 읽어 오기 위한 경로 정보 생성\n",
        "#   - hint : os.path.dirname() 메서드를 이용하여 데이터 셋이 저장된 경로 추출\n",
        "#     => /root/.keras/datasets/rps_test-set.zip => /root/.keras/datasets/\n",
        "\n",
        "train_dir = os.path.dirname(train_path) + '/rps'\n",
        "test_dir = os.path.dirname(test_path) + '/rps-test-set'\n",
        "\n",
        "\n",
        "\"\"\" Step 1-(3) ImageDataGenerator 객체 생성  \n",
        "    - 객체 생성 시 rescale 인자를 이용하여 텐서 내 원소의 범위를 [0 ~ 255] => [0 ~ 1] 로 ReScaling 진행\n",
        "    - 이미지의 사이즈를 VGG16 Backbone 모델에 적합한 (224, 224) 로 지정\n",
        "\"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. ImageDataGenerator API 를 이용하여 학습 데이터와 테스트 데이터를 읽어오기 위한 객체 생성\n",
        "#   - feature 데이터를 [0, 1] 사이로 scailing을 수행하세요\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\"\"\" - flow_from_directory() 메서드를 이용하여 학습데이터와 검증데이터를 위한 DirectoryIterator 객체 생성\"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. ImageDataGenerator 객체의 flow_from_directory 메서드를 이용하여 데이터를 읽어오기 위한 정보를 설정하세요\n",
        "#   - 이미지의 사이즈를 VGG16 Backbone 모델에 적합한 (224, 224) 로 지정하세요\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    directory=train_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=20,\n",
        "    shuffle=True,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    directory=test_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=20,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" Step 2. VGG16을 Backbone 으로 하는 모델 디자인 및 학습 정보 설정\"\"\"\n",
        "\n",
        "\"\"\" Step 2-(1) Pre-trained 된 VGG16 모델 객체 생성\n",
        "    - imagenet 데이터를 이용해 학습된 모델 객체 생성\n",
        "    - classification layer 제외\n",
        "\"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. tensorflow keras 에서 제공하는 이미 학습된 VGG16 Backbone 모델 객체를 생성하세요\n",
        "#   - imagenet 데이터를 이용해 학습된 Backbone 모델 객체를 생성하세요\n",
        "#   - classification layer 제외하여 VGG16 모델의 Backbone 객체만 생성하세요\n",
        "#   - 이미지의 사이즈를 VGG16 Backbone 모델에 적합한 (224, 224) 로 지정하세요\n",
        "from tensorflow.keras.applications import VGG16\n",
        "conv_base = VGG16(weights='imagenet',\n",
        "                  include_top=False,\n",
        "                  input_shape=(224, 224, 3))\n",
        "\n",
        "\n",
        "\"\"\" Step 2-(2) VGG16 Backbone 모델에 classification layer 추가\"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. Sequential API를 이용하여 가위-바위-보 데이터셋 을 분석 하기 위한 CNN 모델을 디자인 하세요\n",
        "#   - VGG16의 Backbone 모델에 classification layer를 직접 추가하여 모델을 디자인 하세요\n",
        "model = keras.models.Sequential()\n",
        "\n",
        "model.add(conv_base)\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dense(units=512, activation='relu'))\n",
        "model.add(keras.layers.Dense(units=3, activation='softmax'))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "\"\"\" Step 3. 모델의 학습 정보 설정\"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. tf.keras.Model 객체의 compile 메서드를 이용하여 학습을 위한 정보들을 설정하세요\n",
        "#   - optimizer : 기 학습된 VGG16 Backbone을 사용하기 때문에 optimizer 의 학습율을 작게 지정 하세요\n",
        "#   - loss\n",
        "#   - metrics : 체점 기준인 accuracy 로 설정\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=2e-5),\n",
        "              loss='categorical_crossentropy' ,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\"\"\" Step 4. 모델에 데이터 generator 연결 후 학습\"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. tf.keras.Model 객체의 fit 메서드를 이용하여 모델을 학습하세요\n",
        "#   - fit 메서드의 verbose=2 로 설정 하세요\n",
        "\n",
        "history = model.fit(\n",
        "          train_generator,\n",
        "          steps_per_epoch=len(train_generator),\n",
        "          epochs=5, verbose=2,\n",
        "          validation_data=test_generator,\n",
        "          validation_steps=len(test_generator))\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc =history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss =history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "\n",
        "\"\"\"* 모델 제출 \"\"\"\n",
        "\n",
        "# 학습된 모델을 제출하기 위한 코드 입니다. 수정하지 마세요\n",
        "model.save('my_model.h5')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}